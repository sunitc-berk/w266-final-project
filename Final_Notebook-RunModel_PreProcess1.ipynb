{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa1eed7",
   "metadata": {},
   "source": [
    "#  Running our models on the How2/WikiHow/CNN data. \n",
    "\n",
    "Following are the high level steps we are following in this notebook:\n",
    "* **Load Test Data :** Summary provided with the article.  \n",
    "* **Use PreProcessed1 data  :**  Pre-Processed 1 data has following details:\n",
    " * Remove Special Characters from Text\n",
    " * Remove Stop Words from Text\n",
    " * Lemmatize Text\n",
    "* **Execute following Models  :**  We are executing multiple models including:\n",
    " * Extractive Summary Model (BERT)\n",
    " * Abstractive Summary Model (BERT2BERT for CNN/Dailymail)\n",
    " * Abstractive T5 Model (pre-trained model that was trained on our data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260a934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "##############\n",
    "## INSTALLS ##\n",
    "##############\n",
    "\n",
    "#!pip install bert-extractive-summarizer\n",
    "#!pip install transformers\n",
    "#!pip install neuralcoref\n",
    "#!pip install datasets==1.0.2\n",
    "#!pip install git-python==1.0.3\n",
    "#!pip install sacrebleu==1.4.12\n",
    "#!pip install rouge_score\n",
    "#!pip install rouge-metric\n",
    "\n",
    "#!pip install rouge\n",
    "#!pip install py-rouge\n",
    "#!pip install pyrouge\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece\n",
    "#!pip install nlp\n",
    "\n",
    "#!python -m nltk.downloader all\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30a36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_md\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import csv\n",
    "import rouge\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "#from rouge_score import rouge_scorer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from tqdm import tqdm_pandas\n",
    "from tqdm import tqdm\n",
    "from summarizer import Summarizer\n",
    "from simplet5 import SimpleT5\n",
    "from datetime import datetime\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2e99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "###############\n",
    "# GLOBAL VARS #\n",
    "###############\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "start_time = datetime.now()\n",
    "aggregator='Avg'\n",
    "apply_avg = aggregator == 'Avg'\n",
    "apply_best = aggregator == 'Best'\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")  \n",
    "abstractive_summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
    "extractive_summarizer_model = Summarizer()\n",
    "modelt5 = SimpleT5()\n",
    "modelt5.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9325fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c08ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>summary</th>\n",
       "      <th>article</th>\n",
       "      <th>data_source</th>\n",
       "      <th>article_pp1</th>\n",
       "      <th>article_pp2</th>\n",
       "      <th>article_pp3</th>\n",
       "      <th>num_words_article</th>\n",
       "      <th>num_sentences_article</th>\n",
       "      <th>num_words_summary</th>\n",
       "      <th>num_sentences_summary</th>\n",
       "      <th>num_words_article_pp1</th>\n",
       "      <th>num_sentences_article_pp1</th>\n",
       "      <th>num_words_article_pp2</th>\n",
       "      <th>num_sentences_article_pp2</th>\n",
       "      <th>num_words_article_pp3</th>\n",
       "      <th>num_sentences_article_pp3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>learn about how hand washing can help prevent ...</td>\n",
       "      <td>hi ! this is david jackel on behalf of expert ...</td>\n",
       "      <td>How2</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>most colds come from direct conotact that you ...</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>359</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>284</td>\n",
       "      <td>11</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>how to julienne cucumbers to make kimchi for k...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>How2</td>\n",
       "      <td>way cucumber also nice pickling cucumber find ...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>way cucumber also nice cucumber find work best...</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            summary  \\\n",
       "2           2  learn about how hand washing can help prevent ...   \n",
       "3           3  how to julienne cucumbers to make kimchi for k...   \n",
       "\n",
       "                                             article data_source  \\\n",
       "2  hi ! this is david jackel on behalf of expert ...        How2   \n",
       "3  the other way we can do cucumbers which is als...        How2   \n",
       "\n",
       "                                         article_pp1  \\\n",
       "2  cold come direct contact somebody else virus o...   \n",
       "3  way cucumber also nice pickling cucumber find ...   \n",
       "\n",
       "                                         article_pp2  \\\n",
       "2  most colds come from direct conotact that you ...   \n",
       "3  the other way we can do cucumbers which is als...   \n",
       "\n",
       "                                         article_pp3  num_words_article  \\\n",
       "2  cold come direct contact somebody else virus o...                359   \n",
       "3  way cucumber also nice cucumber find work best...                171   \n",
       "\n",
       "   num_sentences_article  num_words_summary  num_sentences_summary  \\\n",
       "2                     14                 20                      2   \n",
       "3                      6                 26                      2   \n",
       "\n",
       "   num_words_article_pp1  num_sentences_article_pp1  num_words_article_pp2  \\\n",
       "2                    123                          1                    284   \n",
       "3                     62                          1                    169   \n",
       "\n",
       "   num_sentences_article_pp2  num_words_article_pp3  num_sentences_article_pp3  \n",
       "2                         11                    116                          1  \n",
       "3                          6                     56                          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# DATA #\n",
    "########\n",
    "\n",
    "\n",
    "# setting number of rows to low number so notebook runs in minutes and not hours. \n",
    "num_rows_each_df = 10\n",
    "\n",
    "cnn_dailymail_df = pd.read_csv(os.getcwd() + \"/data/cnn_dm_df.csv\",encoding = \"utf-8\")\n",
    "wikihow_df = pd.read_csv(os.getcwd() + \"/data/wikihow_df.csv\",encoding = \"utf-8\")\n",
    "how2_df = pd.read_csv(os.getcwd() + \"/data/how2_df.csv\",encoding = \"utf-8\")\n",
    "\n",
    "wikihow_df = wikihow_df[(wikihow_df.article_pp1.str.len() < 3700) & (wikihow_df.summary.str.len() > 100)]\n",
    "how2_df = how2_df[(how2_df.article_pp1.str.len() < 3700) & (how2_df.summary.str.len() > 100)]\n",
    "cnn_dailymail_df = cnn_dailymail_df[(cnn_dailymail_df.article_pp1.str.len() > 250) & (cnn_dailymail_df.summary.str.len() > 100)]\n",
    "\n",
    "if len(wikihow_df) > num_rows_each_df:\n",
    "    wikihow_df = wikihow_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(how2_df) > num_rows_each_df:\n",
    "    how2_df = how2_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(cnn_dailymail_df) > num_rows_each_df:\n",
    "    cnn_dailymail_df = cnn_dailymail_df.head(num_rows_each_df)\n",
    "    \n",
    "merged_df = pd.concat([how2_df, wikihow_df,cnn_dailymail_df], axis=0)\n",
    "merged_df = merged_df[merged_df.article_pp1.str.len() > 250]\n",
    "\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f0b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51419e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "def RemoveIntroFromText(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"the sentence is likely an introduction\")\n",
    "            new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def RemoveIntroFromTextMiddle(text):\n",
    "    script = nlp(text)\n",
    "    sentences = [x for x in script.sents]\n",
    "    #print(\"sentences.....\")\n",
    "    #print(sentences)\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"skipping the sentence as it is likely an introduction\")\n",
    "            #new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def RemoveIntroFromTextNonVerbose(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\" \n",
    "    #displacy.render(script, jupyter=True, style='ent') \n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        if len(d)>0:\n",
    "            for key in d:\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "             new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                 new_text+=\". \"\n",
    "        i+=1\n",
    "    return new_text\n",
    "\n",
    "\n",
    "#Raw Text Summarization\n",
    "def generate_abstractive_summary(raw_string, model = abstractive_summarizer_model, max_length=512):\n",
    "    \"\"\"This function produces an abstractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An abstractive summarizer model\"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(raw_string, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_str[0]\n",
    "\n",
    "\n",
    "def generate_extractive_summary(raw_string, model = extractive_summarizer_model, min_summary_length = 50):\n",
    "    \"\"\"This function produces an extractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An extractive summarizer model\"\"\"\n",
    "    output_str = model(raw_string, min_length = min_summary_length)\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def process_article(text):\n",
    "    #print(\"proces article\")\n",
    "    article = text.split(\".\")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        #print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(in_text, top_n=5):\n",
    "    summarize_text = []\n",
    "    try:\n",
    "        # Step 1 - Read text anc split it\n",
    "        sentences =  process_article(in_text)\n",
    "        # Step 2 - Generate Similary Martix across sentences\n",
    "        sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "        # Step 3 - Rank sentences in similarity martix\n",
    "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        # Step 4 - Sort the rank and pick top sentences\n",
    "        ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "        #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "        for i in range(top_n):\n",
    "            summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "        # Step 5 - Offcourse, output the summarize text\n",
    "        #print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "    except:\n",
    "        return \"\"\n",
    "    finally:\n",
    "        return \". \".join(summarize_text)\n",
    "\n",
    "def generate_abstractive_summary_T5(raw_string):\n",
    "    # using epoch 5\n",
    "    modelt5.load_model(\"t5\",\"outputs/simplet5-epoch-7-train-loss-0.9977\", use_gpu=False)\n",
    "    return modelt5.predict(raw_string)[0]\n",
    "\n",
    "#def prepare_results(p, r, f):\n",
    "#    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "def print_rogue_scores(hypo, refe):\n",
    "    scores = evaluator.get_scores(hypo, refe)\n",
    "    #scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "            for hypothesis_id, results_per_ref in enumerate(results):\n",
    "                nb_references = len(results_per_ref['p'])\n",
    "                for reference_id in range(nb_references):\n",
    "                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "                    print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results_per_ref['p'][reference_id], 'R', 100.0 * results_per_ref['r'][reference_id], 'F1', 100.0 * results_per_ref['f'][reference_id]))\n",
    "                    #print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "            print()\n",
    "        else:\n",
    "            print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))\n",
    "            #print(\"x\") #prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32a47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Rouge Evaluator  #\n",
    "####################\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b9373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Len= 805\n",
      "cold come direct contact somebody else virus often time like shaking hand somebody close quarter hugging touching anything close somebody else sharing thing touching glass touching silverware touching food stuff like constantly washing hand especially traveling close quarter people exposed someone might sick always washing hand warm water soap wash vigorously least 20 second make sure loosen germ wo nt always access warm water soap carry hand sanitizer always keep hand sanitizer car bag traveling something take whenever need put little bit hand shaking hand people public event nt know sick shaking hand someone else sick really worth getting sick use hand sanitizer important thing remember hand vessel germ reach body always keep hand frequently possible also always wash hand eating touching face\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "a-summary= hand sanitizer essential thing to stop people from washing hand vessels. no need to wash your hands... even if you're sick of a cold come direct contact with your hand. hand port body is traveling something take whenever need put your hand in hand. use the restroom and go to the bathroom for a few minutes.\n",
      "----------------\n",
      "t5-summary= someone else virus often time like shaking hands people public event nt know sick shaking hand someone else sharing thing touching glass touching silverware touching anything close somebody else sharing thing touching glass touching silverware touching food stuff like constantly washing hand always keep hand sanitizer always keep hand sanitizer always carry hand sanitizer when traveling something take whenever need put little bit hand shaking hand people public event nt know sick shaking hand someone else sick really worth getting sick use hand sanitizer when traveling something take whenever need\n",
      "----------------\n",
      "ss-summary= . sanitizer when traveling something take whenever need put hand sanitizer when traveling something take whenever need.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 360\n",
      "way cucumber also nice pickling cucumber find work best firmer slice diagonally thing 18 14 cut little bit thicker going use style well see going turn side going cut strip one pickled going make nice soft flexible still good crunch careful cut finger ca nt enjoy meal finger missing thing end going bring bowl going mix together see process matter cut cucumber\n",
      "----------------\n",
      "e-summary= way cucumber also nice pickling cucumber find work best firmer slice diagonally thing 18 14 cut little bit thicker going use style well see going turn side going cut strip one pickled going make nice soft flexible still good crunch careful cut finger ca nt enjoy meal finger missing thing end going bring bowl going mix together see process matter cut cucumber\n",
      "----------------\n",
      "a-summary= the way cucumber picks up your pickling is good for you. take a look at the best way to pick up your favorite pickling. take the pickled in the middle of the line... and take a turn turn side - by - side. try the same technique as the other one.\n",
      "----------------\n",
      "t5-summary= pickling cucumber find work best firmer slice go side going cut little bit thicker going use style well see going 14 cut little bit thicker going use style well see going good crunch nice pickling cucumber also nice pickling cucumber find work best firmer slice enjoy meal finger missing thing end going bring bowl going mix together see process matter cut cucumber also nice pickling cucumber find work best firmer slice a little bit thicker going use style well see going go way side going cut strip one pickled going make nice also\n",
      "----------------\n",
      "ss-summary= .pickling cucumber find work best firmer slice go side going go good crunch nice pickling cucumber also nice pickling cucumber find work best firmer slice enjoy meal finger missing thing end going bring bowl going mix together see process matter cut cucumber also nice pickling cucumber find work best firmer slice a little bit thicker going use style well see going go way side going cut strip one pickled going make nice soft flexible still good crunch nice pickling cucumber also nice pickling cucumber find work best firmer slice enjoy meal finger missing thing end\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 415\n",
      "photograph emulsion heat emulsion light tight space using red photographic safe light paint photographic emulsion onto watercolor paper print using traditional photographic technique dark room like especially make photograph nt necessarily flat plastic put watercolor paper make card make even really beautiful painterlyesque image one way kind create style using photographic emulsion painted onto watercolor paper\n",
      "----------------\n",
      "e-summary= photograph emulsion heat emulsion light tight space using red photographic safe light paint photographic emulsion onto watercolor paper print using traditional photographic technique dark room like especially make photograph nt necessarily flat plastic put watercolor paper make card make even really beautiful painterlyesque image one way kind create style using photographic emulsion painted onto watercolor paper\n",
      "----------------\n",
      "a-summary= emulsion painted onto watercolor paper print using traditional photographic technique dark room. use traditional photographic techniques to create style using traditional techniques darkroom paper print uses traditional photographic methods dark room techniques dark room like make - or - make card make card and make card. use this technique to create a style using photographic emulsion paper print.\n",
      "----------------\n",
      "t5-summary= emulsion heat emulsion light tight space using red photographic safe light paint photographic emulsion onto watercolor paper print using traditional photographic technique dark room like especially make photograph use traditional photographic technique dark room like especially make card make even really beautiful painterlyesque image one way kind create style using photographic emulsion painted onto watercolor paper print using traditional photographic technique dark room like especially make card make card make card make card make card make card make card make card make card make card make card make card make card make card make card make card\n",
      "----------------\n",
      "ss-summary= emulsion painted onto watercolor paper print using traditional photographic techniques dark room like especially make photograph nt necessarily flat plastic put watercolor paper make card.emulsion painted onto watercolor paper print using traditional photographic techniques dark room like especially make photograph use traditional photographic techniques dark room like especially make card.emulsion painted onto watercolor paper print using traditional photographic techniques dark room like especially make photograph nt necessarily flat plastic put watercolor paper make card.use traditional photographic techniques to create style using traditional photographic techniques dark room like especially make card.use traditional photographic techniques to create\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 805\n",
      "episode actually going use interesting technique combine bleeding acupuncture along cupping commonly used heat syndrome well pain great choice back pain going swab area make sure sterile case use lancet similar diabetic use checking blood sugar utilize cup fashion place location use bleeding needle maybe able see camera starting get drop blood starting come forward leave minute amount blood start increase give moment work go actually starting get little bit drip good actually order therapeutic oftentimes leave five minute get teaspoon blood case magic tv go ahead leave quite long see release pressure cup blood actually kinda spray cup cup screamish might want go ahead watch video right ahh nt much see little bit blood go ahead clean little bit bruising left bleeding cupping quite effective pain\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "a-summary= the episode actually going use interesting technique combine bleeding acupuncture along cupping commonly used heat syndrome as well as blood sugar and blood sugar levels. the episode will be shown on cnn at 8 p. m. et on thursday at 10pm et / pt / ptt.\n",
      "----------------\n",
      "t5-summary= bleeding needles for back pain blood pressure cup little bit blood go ahead clean little bit bruising left bleeding cupping quite effective pain relief get little bit blood use interesting technique good choice back pain really going use interesting technique combine bleeding acupuncture along with cupping commonly used heat syndrome well pain great choice back pain actually going use interesting technique combine bleeding acupuncture along with cupping commonly used heat syndrome well pain great choice back pain really going use interesting technique combine bleeding acupuncture along with cupping oftentimes relieve\n",
      "----------------\n",
      "ss-summary= ..the episode actually going use interesting technique combine bleeding acupuncture along with cupping commonly used heat syndrome as well as blood sugar and blood sugar levels.the episode will be shown on cnn at 8 p. m. et on thursday at 10pm..bleeding needles for back pain blood pressure cup little bit blood go ahead clean little bit blood go ahead clean little bit bruising left bleeding cupping quite effective pain relief get little bit blood use interesting technique combine bleeding acupuncture\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 417\n",
      "alright terminology ball strike call two ball one strike course everyone know get three strike four ball take base hit pitch take base see clicker indicator see game ended 11 count 2 out let say game went watch ball two strike two ball three call full count ball four take base go back zero start make sure reset indicator every batter otherwise could start 40 nt work reset 00see 00 ball 1 strike 1 strike 2 strike 3\n",
      "----------------\n",
      "e-summary= alright terminology ball strike call two ball one strike course everyone know get three strike four ball take base hit pitch take base see clicker indicator see game ended 11 count 2 out let say game went watch ball two strike two ball three call full count ball four take base go back zero start make sure reset indicator every batter otherwise could start 40 nt work reset 00see 00 ball 1 strike 1 strike 2 strike 3\n",
      "----------------\n",
      "a-summary= ball strike four take base take base go back zero start 11. take base three take base one strike four strike four. catch a strike with a strike on the third strike. take on a strike for a strike two strike. click here for all the latest from the game.\n",
      "----------------\n",
      "t5-summary= two strike call full count ball four take base hit pitch take base hit take base hit return zero start make sure reset indicator every batter otherwise could start 40 nt work reset 00see 00 ball one strike 1 strike 2 strike 3 strike 4 strike take base hit pitch take base hit take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base hit pitch take base\n",
      "----------------\n",
      "ss-summary= take base hit. strike four. three strike four. take base hit pitch take base hit return zero start. take base hit. base hit. base hit base hit. base hit. base hit. take base hit. base hit. base two take base one strike two. catch a strike with a strike on the third strike. three strike four. out let say game went watch. 1 3 strike 4. take base hit.. take base hit.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 1033\n",
      "click right say kind case got radiant 200 series one thing know laptop video card different kind simple looking right dell system planning kind gaming laptop really going find laptop video card cable play like high game 1000 dollar actually 11 12 maybe even 13 case dell going buy couple option intel graphite meaning accelerator 950 good enough game mostly intended email stuff like typing letter second option dell laptop 128 mega mobility s1300 would play game 256 mega radian x1400 would also play game card last two able play game first first one would play game another manufacture right called cyber power ati radian one make video card also nvidia case cyber power cyber power come come mgva force 7600 256 mega better dell also 1200 dollar planning playing kind high game lot pay like 700 dollar going play game another thing like said video card laptop upgradeable kind one kind laptop could upgrade video called mxm module laptop module basically look like super nintendo cartilage pop pop new one popular gone circulation\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "a-summary= laptops can play game of the day, as well as super nintendo's mega power ati radian x1400 would play game card last two able playbook s1300 would also play a game of super nintendo cartilage pop pop pop new one. the gadget could upgrade video game consoles to $ 1. 2 billion.\n",
      "----------------\n",
      "t5-summary= right say kind case got radiant 200 series one thing know laptop video card different kind simple looking right dell also 128 mega mobility s1300 good enough game mostly intended email stuff like typing letter second option dell going buy couple option intel graphite meaning accelerator 950 good enough game mostly intended email stuff like typing letter second option dell also 128 mega mobility s1300 good enough game mostly intended email stuff like typing letter second option dell also 1200 dollar planning playing kind gaming laptop really going find laptop video card cable play like\n",
      "----------------\n",
      "ss-summary= ..just say kind case got radiant 200 series one thing know laptop video card different kind simple looking right dell also 128 mega mobility s1300 good enough game mostly intended email stuff like typing letter second option dell going buy couple option intel graphite meaning accelerator 950 good enough game mostly intended email stuff like typing letter second option dell also 1200 dollar planning playing kind gaming laptop really going find laptop video card cable play like.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 395\n",
      "well think damage well standard tuning going 95 rest would depend going pulling thing expensive pulling thing free sometimes really find really interesting thing could get point take look going sound good first thing going pull desk want grab plant sure find certain thing piano 5 foot 7 look 20 designed cabinet appearance sound board thing going specific work tell pin still manufacture height\n",
      "----------------\n",
      "e-summary= well think damage well standard tuning going 95 rest would depend going pulling thing expensive pulling thing free sometimes really find really interesting thing could get point take look going sound good first thing going pull desk want grab plant sure find certain thing piano 5 foot 7 look 20 designed cabinet appearance sound board thing going specific work tell pin still manufacture height\n",
      "----------------\n",
      "a-summary= take a look at some of the most interesting things in the world. check out what you can find on a pull desk. find out what to do with a piano 5 foot 7 7 look 20 feet 7. look for a guitar 5 - foot 7 inch 7. make sure you don't need to make any noise noise.\n",
      "----------------\n",
      "t5-summary= think damage well sound board thing going pull thing expensive pulling thing cheap look going good first thing going pull thing look going looking look 7 look 20 designed cabinet appearance sound board thing going pull thing get go find certain thing going pull thing kind of like that thing going 5 foot 7 look 20 designed cabinet appearance sound board thing going thing going stuff getting take look going want grab plant look free get get point get get get really get get find\n",
      "----------------\n",
      "ss-summary= think damage well standard tuning going 95 rest would depend. look for a pull desk.go go get find certain thing going pull desk..get lookingget get find certain thing going 5 foot 7 inch 7. grab plant getget get get get find. take look at things. get. look 20 feet 7. consider what to do with a pull desk....find something interesting.seegetget get find.get\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 526\n",
      "position foot foundation always want start foot standing pose foundation build strong house want foot length one leg spaced apart always use yoga tie beginning bathrobe belt regular belt necktie measure length outside outside leg outer foot hip place yoga mat determine width foot want make sure foot call parallel often use second toe determine make second toe parallel press firmly foot begin creating firm foundation standing posture case virasana 1 hero 1after awhile able tell distance apart need foot beginning good tool\n",
      "----------------\n",
      "e-summary= position foot foundation always want start foot standing pose foundation build strong house want foot length one leg spaced apart always use yoga tie beginning bathrobe belt regular belt necktie measure length outside outside leg outer foot hip place yoga mat determine width foot want make sure foot call parallel often use second toe determine make second toe parallel press firmly foot begin creating firm foundation standing posture case virasana 1 hero 1after awhile able tell distance apart need foot beginning good tool\n",
      "----------------\n",
      "a-summary= the position foot foundation always want foot length one leg apart. foot start with yoga tie beginning bathrobe belt necktie measure length outside outside leg outer footwear. foot stand foundation always wants foot to be one leg wide. foot foundation foundation is standing posture foundation standing posture case.\n",
      "----------------\n",
      "t5-summary= outside outside leg outer foot hip place yoga mat measure length outside outside outside leg outer foot hip place yoga mat determine make second toe parallel press firmly foot foundation always want start creating firm foundation always want to begin building strong house want foot need to begin measuring distance apart need to begin using good tool for measuring distance between two legs always use first toe measure width outside outside outside outside leg outer foot hip place yoga mat measure length outside outside outside outside leg outer foot hip place yoga mat measure length outside outside outside outside leg outer foot hip place yoga mat measure width outside\n",
      "----------------\n",
      "ss-summary= measure length outside outside leg outer footwear. toe determine make second toe parallel press firmly foot foundation always want start creating firm foundation always want foot be one leg wide.the position foot foundation always want start creating firm foundation always want start creating strong house want foot need to begin measuring distance apart need to begin using good tool.measure width outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside outside\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 443\n",
      "going let cook well butter melted going dump straight pan going put oven five minute waiting take pan give little wash going measure one cup water going add two cube vegetable bouillon going using stock chicken ca nt find vegetable bouillon chicken fine well waiting start working asparagus fill pan half way water add tablespoon little le tablespoon salt turn front burner eight take sauté pan getting ready prepare chicken add tablespoon oil\n",
      "----------------\n",
      "e-summary= going let cook well butter melted going dump straight pan going put oven five minute waiting take pan give little wash going measure one cup water going add two cube vegetable bouillon going using stock chicken ca nt find vegetable bouillon chicken fine well waiting start working asparagus fill pan half way water add tablespoon little le tablespoon salt turn front burner eight take sauté pan getting ready prepare chicken add tablespoon oil\n",
      "----------------\n",
      "a-summary= chicken bouillon will be the first dish in the world to be served asparagus. take two cube vegetable bouilly and one cup of chicken cavallon. take a saute pan with two cubes of potatoes and two cube salads... and have a cup of water water.\n",
      "----------------\n",
      "t5-summary= nt find vegetable bouillon going put oven five minute waiting take pan give little wash going measure one cup water going add two cubes vegetable bouillon going measure one cup water going add two cubes vegetable bouillon going using stock chicken fine well waiting start working asparagus fill pan half way water going add tablespoon little le tablespoon salt turn front burner eight take sauté pan getting ready prepare chicken add tablespoon oil to sauté pan adding tablespoon oil to sauté pan taking sauté pan getting ready to cook chicken fine well waiting start working asparagus\n",
      "----------------\n",
      "ss-summary= a saute pan with two cubes of potatoes and two cube salads. water add one cup water add tablespoon oil to sauté pan taking sauté pan getting ready to cook chicken.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 2258\n",
      "youre photographer keep necessary lens cord battery quadrant home studio paint kept brush cleaner canvas print supply ink etc make broader group area supply make finding easier limiting search much smaller area idea include essential supply area thing use every day inspiration reference area dedicated work area infrequent secondary supply area tucked way doesnt mean cleaning entire studio mean keeping area immediately around desk easel pottery wheel etc clean night discard trash unnecessary material wipe dirty surface endeavor leave workspace way sit next day start working immediately without work tidying even rest studio bit disorganized organized workspace help get business every time want make art visual people lot artist clutter come desire keep track supply visually instead tucked sight using jar old glass vas cheap clear plastic drawer keep thing sight without leaving strewn haphazardly idea beyond mentioned include canvas shoe rack back door wine rack cup slot hold penspencils plastic restaurant squirt bottle paint pigment etc simply string wire across wall along ceiling use hold essential paper dont want cut ruin tack tape cheap easy also good way handle paper idea touch regularly need pin inspiration shelving artist best friend cheap easy way get room studio art space dont afraid get high either especially infrequently used supply upper reach room often underutilized provide vital space tool material turning one wall chalkboard give perfect space idea sketch planning without requiring extra equipment space even use smaller area paint jar storage equipment allowing relabel chalk need change lot disorganization come keep moving location thing trying optimize space reorganizing frequently usually opposite effect leading lost item uncertainty cleaning afternoon label maker solve everything instead spending mental energy looking storing thing follow label freeing mind think art month purge studio isnt essential part project either throw file away later artist constantly making new thing experimenting making mess good thing set aside time declutter may fun moment lot fun spending 30 minute digging junk find right paint old sketch dont sentimental havent used last six month little chance youll use next six month toss\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "a-summary= youre photographer keep vital supply area infrequent secondary supply area. take a look at every day for work every time want make art visual people lot workspace way around desks. use the studio to make a mess good and keep track of the rest of the time.\n",
      "----------------\n",
      "t5-summary= photographer tucked away in the closet organize essential supply area thing file away later artist file away later file away later artist constantly making new thing month month little chance youll use next six month toss month month little chance youll use next six month toss month month month small storage simple keep things sight without leaving strewn use time work tidy change habit move clutter free mind good month little chance month\n",
      "----------------\n",
      "ss-summary= . essential supply area infrequent secondary supply area.keep the studio tidy and organized..Keep everything tucked away in the closet.. month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance month..month little chance\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 2498\n",
      "see image drawing develops stepbystep however important detail following drawing examine create something unique use line create image shape section fill appeared section different pattern ornament add text needed example neopoprealism 25 add colored strip top color wish painting mural always requires preparation ‘ need equipment effort planning attention detail help succeed painting mural requires suitable location right surface painted surface smooth flat however even roughtextured surface used neopoprealist mural project exterior project last year using newer 100 acrylic exterior paint would best choice interior wall use latex paint latex offer easier cleanup lower cost measuring total wall area covered total amount paint calculated since mural painting requires two color white black figuring actual area painted color necessary allow purchasing right amount one large wall background may rolled sprayed white paint sprayer detail may added brush paint sensitive high temperature humidity direct sunlight however interior project many complication public place keeping mural protected may require attention reason make neopoprealist mural dedicated 25year anniversary school office consider using varnish mural see sample design give sense proportion unique requirement element use sketch measure scale distance location various point subject measuring key feature help calculate amount paint feature identified color surface low whole mural painted standing ground stepladder higher work may rent scaffold mark horizontal vertical line use white paint background begin marking using scaled sketch location key element object located foreground everything depends complexity mural confident artistic result may choose draw detail freehand careful keep clean transition edge one color black another white however mistake touched later always allow fresh color dry proceeding drawing example would painting large pattern use big brush limb use small brush tiny detailed pattern drip run paint color paint appropriate location sharpen line pattern blurred intended last long time surface require cleaning overcoat mural project clear sealer however wont able involve 16 percent brain gray matter youll end primitive crafting pattern even worse doodling socalled zendoodling create neopoprealist art one need ability developed talented people studying using nadia rus neopoprealist instructional book copycat selfpromotional superficial book teach doodle nothing common visual art mission\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "a-summary= the work of neopoprealist muralrealist graffiti project last year. it uses newer 100 acrylic exterior paint and paint latex paint. the project is expected to be completed by the end of this year using more than 100 paint paint. it is not known if the project will be used in the future.\n",
      "----------------\n",
      "t5-summary= detail important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important\n",
      "----------------\n",
      "ss-summary= .the work of neopoprealist muralrealist graffiti project last year.it uses newer 100 acrylic exterior paint and paint latex paint.detail important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines stepbystep however important detail following drawing examines\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TEST Loop for Abstractive and Extractive summarization\n",
    "\n",
    "icount = 0\n",
    "\n",
    "for article in merged_df['article_pp1']:\n",
    "    if len(article) > 200:\n",
    "        print(\"Article Len=\", len(article))\n",
    "        print(article)\n",
    "        e_summary = generate_extractive_summary(article, min_summary_length=50)\n",
    "        a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "        t5_summary = generate_abstractive_summary_T5(article)\n",
    "        all_summary = e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "        s_s_summary = generate_abstractive_summary_T5(all_summary)\n",
    "        \n",
    "        print(\"----------------\")    \n",
    "        print(\"e-summary=\",e_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"a-summary=\",a_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"t5-summary=\",t5_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"ss-summary=\",s_s_summary)\n",
    "        print(\"-------------------------------------------------------------------------------------------------\") \n",
    "        icount +=1\n",
    "\n",
    "        if icount > 10:\n",
    "            break \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5ff385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,Dataframe len= 10\n",
      "e_summ len= 29\n",
      "a_summ len= 29\n",
      "t5_summ len= 29\n",
      "ss_summ len= 29\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "e_list = []\n",
    "a_list = []\n",
    "t5_list = []\n",
    "sum_sum_list = []\n",
    "\n",
    "iCount = 0\n",
    "\n",
    "for article in merged_df['article_pp1']:\n",
    "    #print(article)\n",
    "    print(iCount, end=\",\")\n",
    "    iCount =  iCount + 1\n",
    "    \n",
    "    e_summary = generate_extractive_summary(article, min_summary_length=100)\n",
    "    a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "    t5_summary = generate_abstractive_summary_T5(article)\n",
    "    all_summary = e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "    s_s_summary = generate_abstractive_summary(all_summary, model = abstractive_summarizer_model)\n",
    "    \n",
    "    e_list.append(e_summary)\n",
    "    a_list.append(a_summary)\n",
    "    t5_list.append(t5_summary)\n",
    "    sum_sum_list.append(s_s_summary)\n",
    "\n",
    "\n",
    "print(\"Dataframe len=\", len(how2_df))\n",
    "print(\"e_summ len=\", len(e_list))\n",
    "print(\"a_summ len=\", len(a_list))\n",
    "print(\"t5_summ len=\", len(t5_list))\n",
    "print(\"ss_summ len=\", len(sum_sum_list))\n",
    "\n",
    "merged_df['e_summarization'] = e_list\n",
    "merged_df['a_summarization'] = a_list\n",
    "merged_df['t5_summarization'] = t5_list\n",
    "merged_df['ss_summarization'] = sum_sum_list\n",
    "\n",
    "merged_df.to_csv(os.getcwd() + \"/data/merged_df_with_Summarization.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ba03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a155e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue for Extractive Summarization\n",
      "\t\trouge-1:\tP:  3.80\tR:  2.15\tF1:  2.60\n",
      "\t\trouge-2:\tP:  0.34\tR:  0.31\tF1:  0.33\n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP:  3.73\tR:  2.23\tF1:  2.70\n",
      "\t\trouge-w:\tP:  2.20\tR:  0.52\tF1:  0.81\n",
      "Rogue for Abstractive Summarization\n",
      "\t\trouge-1:\tP: 25.31\tR: 20.91\tF1: 21.75\n",
      "\t\trouge-2:\tP:  3.61\tR:  3.15\tF1:  3.21\n",
      "\t\trouge-3:\tP:  0.07\tR:  0.07\tF1:  0.07\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP: 23.93\tR: 19.60\tF1: 20.76\n",
      "\t\trouge-w:\tP: 14.07\tR:  4.95\tF1:  7.02\n",
      "Rogue for T5 Summarization\n",
      "\t\trouge-1:\tP: 21.95\tR: 12.69\tF1: 15.13\n",
      "\t\trouge-2:\tP:  3.14\tR:  1.83\tF1:  2.19\n",
      "\t\trouge-3:\tP:  0.60\tR:  0.29\tF1:  0.37\n",
      "\t\trouge-4:\tP:  0.08\tR:  0.05\tF1:  0.06\n",
      "\t\trouge-l:\tP: 21.99\tR: 13.38\tF1: 15.91\n",
      "\t\trouge-w:\tP: 12.91\tR:  2.97\tF1:  4.62\n",
      "Rogue for SS Summarization\n",
      "\t\trouge-1:\tP: 24.44\tR: 21.25\tF1: 21.63\n",
      "\t\trouge-2:\tP:  2.83\tR:  2.77\tF1:  2.65\n",
      "\t\trouge-3:\tP:  0.03\tR:  0.06\tF1:  0.04\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP: 23.37\tR: 19.83\tF1: 20.69\n",
      "\t\trouge-w:\tP: 13.52\tR:  5.01\tF1:  7.01\n"
     ]
    }
   ],
   "source": [
    "#how2_df['e_summarization'] = e_list\n",
    "#how2_df['a_summarization'] = a_list\n",
    "#how2_df['t5_summarization'] = t5_list\n",
    "\n",
    "hypo=merged_df['summary'].tolist()\n",
    "refe1=merged_df['e_summarization'].tolist() #[reference]\n",
    "refe2=merged_df['a_summarization'].tolist() #[reference]\n",
    "refe3=merged_df['t5_summarization'].tolist() #[reference]\n",
    "refe6=merged_df['ss_summarization'].tolist() #[reference]\n",
    "\n",
    "print(\"Rogue for Extractive Summarization\")\n",
    "print_rogue_scores(hypo,refe1)    \n",
    "print(\"Rogue for Abstractive Summarization\")\n",
    "print_rogue_scores(hypo,refe2)   \n",
    "print(\"Rogue for T5 Summarization\")\n",
    "print_rogue_scores(hypo,refe3)        \n",
    "print(\"Rogue for SS Summarization\")\n",
    "print_rogue_scores(hypo,refe6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75bf487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0d7cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:13:23.527883\n"
     ]
    }
   ],
   "source": [
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef9fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d977b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
