{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa1eed7",
   "metadata": {},
   "source": [
    "#  Running our models on the How2/WikiHow/CNN data. \n",
    "\n",
    "Following are the high level steps we are following in this notebook:\n",
    "* **Load Test Data :** Summary provided with the article.  \n",
    "* **Use \"raw\" article data  :**  No Pre-processing at all of the text. \n",
    "* **Execute following Models  :**  We are executing multiple models including:\n",
    " * Extractive Summary Model (BERT)\n",
    " * Abstractive Summary Model (BERT2BERT for CNN/Dailymail)\n",
    " * Abstractive T5 Model (pre-trained model that was trained on our data). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260a934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "##############\n",
    "## INSTALLS ##\n",
    "##############\n",
    "\n",
    "#!pip install bert-extractive-summarizer\n",
    "#!pip install transformers\n",
    "#!pip install neuralcoref\n",
    "#!pip install datasets==1.0.2\n",
    "#!pip install git-python==1.0.3\n",
    "#!pip install sacrebleu==1.4.12\n",
    "#!pip install rouge_score\n",
    "#!pip install rouge-metric\n",
    "\n",
    "#!pip install rouge\n",
    "#!pip install py-rouge\n",
    "#!pip install pyrouge\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece\n",
    "#!pip install nlp\n",
    "\n",
    "#!python -m nltk.downloader all\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30a36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_md\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import csv\n",
    "import rouge\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "#from rouge_score import rouge_scorer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from tqdm import tqdm_pandas\n",
    "from tqdm import tqdm\n",
    "from summarizer import Summarizer\n",
    "from simplet5 import SimpleT5\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2e99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package stopwords to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "###############\n",
    "# GLOBAL VARS #\n",
    "###############\n",
    "\n",
    "aggregator='Avg'\n",
    "apply_avg = aggregator == 'Avg'\n",
    "apply_best = aggregator == 'Best'\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")  \n",
    "abstractive_summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
    "extractive_summarizer_model = Summarizer()\n",
    "modelt5 = SimpleT5()\n",
    "modelt5.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9325fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c08ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>summary</th>\n",
       "      <th>article</th>\n",
       "      <th>data_source</th>\n",
       "      <th>article_pp1</th>\n",
       "      <th>article_pp2</th>\n",
       "      <th>article_pp3</th>\n",
       "      <th>num_words_article</th>\n",
       "      <th>num_sentences_article</th>\n",
       "      <th>num_words_summary</th>\n",
       "      <th>num_sentences_summary</th>\n",
       "      <th>num_words_article_pp1</th>\n",
       "      <th>num_sentences_article_pp1</th>\n",
       "      <th>num_words_article_pp2</th>\n",
       "      <th>num_sentences_article_pp2</th>\n",
       "      <th>num_words_article_pp3</th>\n",
       "      <th>num_sentences_article_pp3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>learn about how hand washing can help prevent ...</td>\n",
       "      <td>hi ! this is david jackel on behalf of expert ...</td>\n",
       "      <td>How2</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>most colds come from direct conotact that you ...</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>359</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>284</td>\n",
       "      <td>11</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>how to julienne cucumbers to make kimchi for k...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>How2</td>\n",
       "      <td>way cucumber also nice pickling cucumber find ...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>way cucumber also nice cucumber find work best...</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>in order to put photographic emulsion on water...</td>\n",
       "      <td>my name is anthony maddaloni and i 'm going to...</td>\n",
       "      <td>How2</td>\n",
       "      <td>photograph emulsion heat emulsion light tight ...</td>\n",
       "      <td>now photographs have an emulsion on them .and ...</td>\n",
       "      <td>photograph emulsion heat emulsion light tight ...</td>\n",
       "      <td>149</td>\n",
       "      <td>9</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>combining bleeding and cupping methods in acup...</td>\n",
       "      <td>in this episode , we 're actually going to use...</td>\n",
       "      <td>How2</td>\n",
       "      <td>episode actually going use interesting techniq...</td>\n",
       "      <td>in this episode , we 're actually going to use...</td>\n",
       "      <td>episode actually going use interesting techniq...</td>\n",
       "      <td>360</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>346</td>\n",
       "      <td>18</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>what terms are necessary for an umpire to know...</td>\n",
       "      <td>alright , some of the terminology is balls and...</td>\n",
       "      <td>How2</td>\n",
       "      <td>alright terminology ball strike call two ball ...</td>\n",
       "      <td>alright , some of the terminology is balls and...</td>\n",
       "      <td>alright terminology ball strike call two ball ...</td>\n",
       "      <td>177</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            summary  \\\n",
       "2           2  learn about how hand washing can help prevent ...   \n",
       "3           3  how to julienne cucumbers to make kimchi for k...   \n",
       "4           4  in order to put photographic emulsion on water...   \n",
       "5           5  combining bleeding and cupping methods in acup...   \n",
       "6           6  what terms are necessary for an umpire to know...   \n",
       "\n",
       "                                             article data_source  \\\n",
       "2  hi ! this is david jackel on behalf of expert ...        How2   \n",
       "3  the other way we can do cucumbers which is als...        How2   \n",
       "4  my name is anthony maddaloni and i 'm going to...        How2   \n",
       "5  in this episode , we 're actually going to use...        How2   \n",
       "6  alright , some of the terminology is balls and...        How2   \n",
       "\n",
       "                                         article_pp1  \\\n",
       "2  cold come direct contact somebody else virus o...   \n",
       "3  way cucumber also nice pickling cucumber find ...   \n",
       "4  photograph emulsion heat emulsion light tight ...   \n",
       "5  episode actually going use interesting techniq...   \n",
       "6  alright terminology ball strike call two ball ...   \n",
       "\n",
       "                                         article_pp2  \\\n",
       "2  most colds come from direct conotact that you ...   \n",
       "3  the other way we can do cucumbers which is als...   \n",
       "4  now photographs have an emulsion on them .and ...   \n",
       "5  in this episode , we 're actually going to use...   \n",
       "6  alright , some of the terminology is balls and...   \n",
       "\n",
       "                                         article_pp3  num_words_article  \\\n",
       "2  cold come direct contact somebody else virus o...                359   \n",
       "3  way cucumber also nice cucumber find work best...                171   \n",
       "4  photograph emulsion heat emulsion light tight ...                149   \n",
       "5  episode actually going use interesting techniq...                360   \n",
       "6  alright terminology ball strike call two ball ...                177   \n",
       "\n",
       "   num_sentences_article  num_words_summary  num_sentences_summary  \\\n",
       "2                     14                 20                      2   \n",
       "3                      6                 26                      2   \n",
       "4                      9                 54                      3   \n",
       "5                     16                 31                      3   \n",
       "6                     12                 27                      2   \n",
       "\n",
       "   num_words_article_pp1  num_sentences_article_pp1  num_words_article_pp2  \\\n",
       "2                    123                          1                    284   \n",
       "3                     62                          1                    169   \n",
       "4                     56                          1                    124   \n",
       "5                    127                          1                    346   \n",
       "6                     78                          1                    166   \n",
       "\n",
       "   num_sentences_article_pp2  num_words_article_pp3  num_sentences_article_pp3  \n",
       "2                         11                    116                          1  \n",
       "3                          6                     56                          1  \n",
       "4                          8                     48                          1  \n",
       "5                         18                    121                          1  \n",
       "6                         12                     68                          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# DATA #\n",
    "########\n",
    "\n",
    "# setting number of rows to low number so notebook runs in minutes and not hours. \n",
    "num_rows_each_df = 10\n",
    "\n",
    "\n",
    "cnn_dailymail_df = pd.read_csv(os.getcwd() + \"/data/cnn_dm_df.csv\",encoding = \"utf-8\")\n",
    "wikihow_df = pd.read_csv(os.getcwd() + \"/data/wikihow_df.csv\",encoding = \"utf-8\")\n",
    "how2_df = pd.read_csv(os.getcwd() + \"/data/how2_df.csv\",encoding = \"utf-8\")\n",
    "\n",
    "wikihow_df = wikihow_df[(wikihow_df.article_pp1.str.len() < 3700) & (wikihow_df.summary.str.len() > 100)]\n",
    "how2_df = how2_df[(how2_df.article_pp1.str.len() < 3700) & (how2_df.summary.str.len() > 100)]\n",
    "cnn_dailymail_df = cnn_dailymail_df[(cnn_dailymail_df.article_pp1.str.len() > 250) & (cnn_dailymail_df.summary.str.len() > 100)]\n",
    "\n",
    "if len(wikihow_df) > num_rows_each_df:\n",
    "    wikihow_df = wikihow_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(how2_df) > num_rows_each_df:\n",
    "    how2_df = how2_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(cnn_dailymail_df) > num_rows_each_df:\n",
    "    cnn_dailymail_df = cnn_dailymail_df.head(num_rows_each_df)\n",
    "    \n",
    "merged_df = pd.concat([how2_df, wikihow_df,cnn_dailymail_df], axis=0)\n",
    "#merged_df = pd.concat([how2_df, wikihow_df], axis=0)\n",
    "merged_df = merged_df[merged_df.article_pp1.str.len() > 250]\n",
    "#merged_df = merged_df.head(5000)\n",
    "#how2_df = how2_df.head(10)\n",
    "#how2_df = how2_df[(how2_df['num_words'] > 200)] # & (how2_df['num_words'] < 400 )]\n",
    "#wikihow_df = wikihow_df[(wikihow_df['num_words'] > 200)] # & (how2_df['num_words'] < 400 )]\n",
    "#cnn_dailymail_df = cnn_dailymail_df[(cnn_dailymail_df['num_words'] > 200)]# & (how2_df['num_words'] < 400 )]\n",
    "\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f0b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51419e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "def RemoveIntroFromText(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"the sentence is likely an introduction\")\n",
    "            new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def RemoveIntroFromTextMiddle(text):\n",
    "    script = nlp(text)\n",
    "    sentences = [x for x in script.sents]\n",
    "    #print(\"sentences.....\")\n",
    "    #print(sentences)\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"skipping the sentence as it is likely an introduction\")\n",
    "            #new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def RemoveIntroFromTextNonVerbose(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\" \n",
    "    #displacy.render(script, jupyter=True, style='ent') \n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        if len(d)>0:\n",
    "            for key in d:\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "             new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                 new_text+=\". \"\n",
    "        i+=1\n",
    "    return new_text\n",
    "\n",
    "\n",
    "#Raw Text Summarization\n",
    "def generate_abstractive_summary(raw_string, model = abstractive_summarizer_model, max_length=512):\n",
    "    \"\"\"This function produces an abstractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An abstractive summarizer model\"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(raw_string, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_str[0]\n",
    "\n",
    "\n",
    "def generate_extractive_summary(raw_string, model = extractive_summarizer_model, min_summary_length = 50):\n",
    "    \"\"\"This function produces an extractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An extractive summarizer model\"\"\"\n",
    "    output_str = model(raw_string, min_length = min_summary_length)\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def process_article(text):\n",
    "    #print(\"proces article\")\n",
    "    article = text.split(\".\")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        #print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(in_text, top_n=5):\n",
    "    summarize_text = []\n",
    "    try:\n",
    "        # Step 1 - Read text anc split it\n",
    "        sentences =  process_article(in_text)\n",
    "        # Step 2 - Generate Similary Martix across sentences\n",
    "        sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "        # Step 3 - Rank sentences in similarity martix\n",
    "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        # Step 4 - Sort the rank and pick top sentences\n",
    "        ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "        #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "        for i in range(top_n):\n",
    "            summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "        # Step 5 - Offcourse, output the summarize text\n",
    "        #print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "    except:\n",
    "        return \"\"\n",
    "    finally:\n",
    "        return \". \".join(summarize_text)\n",
    "\n",
    "def generate_abstractive_summary_T5(raw_string):\n",
    "    # using epoch 5\n",
    "    modelt5.load_model(\"t5\",\"outputs/simplet5-epoch-7-train-loss-0.9977\", use_gpu=False)\n",
    "    return modelt5.predict(raw_string)[0]\n",
    "\n",
    "#def prepare_results(p, r, f):\n",
    "#    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "def print_rogue_scores(hypo, refe):\n",
    "    scores = evaluator.get_scores(hypo, refe)\n",
    "    #scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "            for hypothesis_id, results_per_ref in enumerate(results):\n",
    "                nb_references = len(results_per_ref['p'])\n",
    "                for reference_id in range(nb_references):\n",
    "                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "                    print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results_per_ref['p'][reference_id], 'R', 100.0 * results_per_ref['r'][reference_id], 'F1', 100.0 * results_per_ref['f'][reference_id]))\n",
    "                    #print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "            print()\n",
    "        else:\n",
    "            print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))\n",
    "            #print(\"x\") #prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32a47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Rouge Evaluator  #\n",
    "####################\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b9373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Len= 1764\n",
      "hi ! this is david jackel on behalf of expert village and i 'm here to talk to you about washing your hands to prevent a cold . now hand washing is actually one of the best ways to prevent catching a cold . the germs that you would catch , travel through your hands onto other parts of your body , specifically your face . most colds come from direct contact that you 're having with somebody else who has the virus . often times that 's like shaking hands with somebody , being in close quarters , hugging , touching , anything when you 're close with somebody else , sharing things , touching the same glass , touching the same silverware , touching the same food , stuff like that . so what you should do is constantly be washing your hands , especially if you 're traveling or if you 're in close quarters with people or you 're exposed to someone who might be sick . always be washing your hands and do it with warm water and soap , wash vigorously for at least 20 seconds to make sure you loosen up all the germs . now , you wo n't always have access to warm water and soap , so what you should do is carry hand sanitizer . i always keep some hand sanitizer with me , in my car , in a bag with me if i 'm traveling and this is something you can take out whenever you need to and just put a little bit of on your hands . if you 're shaking hands with people at a public event , you do n't know who is sick or who is shaking hands with someone else who is sick . it 's really not worth getting sick , so use some hand sanitizer . the important thing to remember is that hands are a vessel through which germs can reach your body , so always keep your hands as frequently as possible . also , always wash your hands before eating and before touching your face .\n",
      "----------------\n",
      "e-summary= this is david jackel on behalf of expert village and i 'm here to talk to you about washing your hands to prevent a cold . most colds come from direct contact that you 're having with somebody else who has the virus .\n",
      "----------------\n",
      "a-summary= the germs that you would catch, travel through your hands onto other parts of your body, specifically your face. hand washing is actually one of the best ways to prevent catching a cold. hand sanitizer is used to wash hands before eating and before touching your face, especially if you're in close quarters with people.\n",
      "----------------\n",
      "t5-summary= always be washing your hands regularly, especially if you're traveling or in close proximity to someone sick, like shaking hands with people or touching food. keep hand sanitizer handy when traveling to prevent catching a cold, as germs can travel through your hands to other parts of the body. wash your hands regularly before touching your face, especially if you're traveling and have a family member who is sick, like me.\n",
      "----------------\n",
      "ss-summary= .wash your hands regularly when traveling.wave your hands regularly, especially if you're traveling with someone sick..keep hand sanitizer handy when traveling..wash your hands regularly before touching your face..wash your hands regularly when traveling to prevent catching a cold..be careful not to touch food or drink while traveling..watch for signs of flu like flu season in the fall and spring..have your hands sanitized at all times..make sure to wash your hands regularly\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 813\n",
      "the other way we can do cucumbers which is also very nice is on these pickling cucumbers i find out that works out the best because they are firmer is to slice them diagonally . same thing about 1/8 '' to 1/4 '' you can cut them a little bit thicker if were going to use this style because well you 'll see why what were going to do once we 're doing this . turn them on the side we 're going to cut them into strips and that ones are pickled it 's going to make them nice soft and flexible but still have the good crunch . be careful not to cut your fingers because you ca n't enjoy your meal if your fingers are missing , same thing with the ends . we 're going to bring our bowl over and i 'm just going to mix them all together because as you see the process is the same no matter how you cut your cucumbers .\n",
      "----------------\n",
      "e-summary= the other way we can do cucumbers which is also very nice is on these pickling cucumbers i find out that works out the best because they are firmer is to slice them diagonally . we 're going to bring our bowl over and i 'm just going to mix them all together because as you see the process is the same no matter how you cut your cucumbers .\n",
      "----------------\n",
      "a-summary= the cucumbers are firmer and flexible but still have the good crunch. you can cut them a little thicker if your fingers are missing. you'll see why what were going to do when we're doing this. the same thing about 1 / 8'to 1 / 4'' you can't cut your fingers '\n",
      "----------------\n",
      "t5-summary= cut your cucumbers in half and slice them diagonally to make them more flexible. chop your cucumbers in half and serve with rice or fish sauce for a side dish.\n",
      "----------------\n",
      "ss-summary= to slice your cucumbers in half. cut your cucumbers in half.......'m going to mix the ingredients in a bowl.... '' chop my cucumbers in half and mix them all together.. to this is so easy add the rice and mix everything together.. combine.........you 'll see why i \n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 787\n",
      "my name is anthony maddaloni and i 'm going to talk about putting photographic emulsion onto watercolor paper . now photographs have an emulsion on them . and what i can do , is i can heat up this emulsion and light , tight space , using a red photographic safe light . and i can paint this photographic emulsion onto watercolor paper . and then i can print using traditional photographic techniques in a dark room . and i like this especially because i can make photographs that are n't necessarily just flat on plastic . i can put them on watercolor paper , i can make cards out of them , i can make even really beautiful painterly-esque images . and so that is one way that i can kind of create a style for myself using photographic emulsion that i 've painted onto watercolor paper .\n",
      "----------------\n",
      "e-summary= my name is anthony maddaloni and i 'm going to talk about putting photographic emulsion onto watercolor paper . and so that is one way that i can kind of create a style for myself using photographic emulsion that i 've painted onto watercolor paper .\n",
      "----------------\n",
      "a-summary= anthony maddaloni has painted emulsions onto watercolor paper. he says he can create a style for himself using a red photographic safe light. he can print using traditional photographic techniques in a dark room. he's working on a way to make a style of his own.\n",
      "----------------\n",
      "t5-summary= anthony maddaloni's technique for putting photographic emulsion onto watercolor paper is very simple. learn how to put photographic emulsion on watercolor paper in this free video on photography.\n",
      "----------------\n",
      "ss-summary= anthony maddaloni's technique for putting photographic emulsion onto watercolor paper is very simple. learn how to put photographic emulsion onto watercolor paper in this free video on photography.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 1637\n",
      "in this episode , we 're actually going to use an interesting technique that combines bleeding acupuncture along with cuppings , very commonly used for heat syndromes as well as for pain . so this is a great choice for back pain . once again , we 're going to swab the area to make sure that it 's sterile . in this case , we use a lancet which is very similar to a diabetic 's use if they 're checking their blood sugar . and now , we utilize the cup again in the same fashion and we 'll place that over the location we 're we just use the bleeding needle . and you maybe able to see from the camera we 're starting to get a drop of blood that 's starting to come forward , and if we leave on here for a few minutes , that amount of blood will start to increase . we 'll give that a moment to do some of its work . here we go . we 're actually starting to get a little bit of a drip from there which is good . and actually , in order to be therapeutic , we oftentimes , we 'll leave this on for five minutes and get about a teaspoon or so of blood out of it . but in this case , for the magic of tv , we 'll go ahead and leave it there not quite so long , and what you 'll see is when i release the pressure from this cup , the blood actually kinda sprays up on the cup , cups . so if you 're screamish you might want to go ahead and not watch the video right now . ahh , we did n't have too much from that . so , you can see that we have a little bit of blood there and we 'll go ahead and just clean that up and a little bit of bruising that 's left over from that . and that 's bleeding , cupping and it 's quite effective for pain .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "e-summary= in this episode , we 're actually going to use an interesting technique that combines bleeding acupuncture along with cuppings , very commonly used for heat syndromes as well as for pain . and actually , in order to be therapeutic , we oftentimes , we 'll leave this on for five minutes and get about a teaspoon or so of blood out of it . but in this case , for the magic of tv , we 'll go ahead and leave it there not quite so long , and what you 'll see is when i release the pressure from this cup , the blood actually kinda sprays up on the cup , cups .\n",
      "----------------\n",
      "a-summary= we're actually starting to get a little bit of blood that's starting to come forward. we'll use the bleeding needle to make sure that it's sterile. the blood actually kinda sprays up on the cup, cups and even a cup of blood out of it.\n",
      "----------------\n",
      "t5-summary= in this video, we're going to show you how to use a bleeding acupuncture technique for back pain. learn how to use a cupping with a lancet for back pain in this free video clip from a chiropractor in this free back pain treatment video.\n",
      "----------------\n",
      "ss-summary= in this video clip, we're going to show you how to use a bleeding acupuncture technique for back pain. learn how to use a cupping with a lancet in this free back pain treatment video.\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TEST Loop for Abstractive and Extractive summarization\n",
    "\n",
    "icount = 0\n",
    "\n",
    "for article in merged_df['article']:\n",
    "    if len(article) > 200:\n",
    "        print(\"Article Len=\", len(article))\n",
    "        print(article)\n",
    "        e_summary = generate_extractive_summary(article, min_summary_length=50)\n",
    "        a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "        t5_summary = generate_abstractive_summary_T5(article)\n",
    "        all_summary = e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "        s_s_summary = generate_abstractive_summary_T5(all_summary)\n",
    "        \n",
    "        print(\"----------------\")    \n",
    "        print(\"e-summary=\",e_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"a-summary=\",a_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"t5-summary=\",t5_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"ss-summary=\",s_s_summary)\n",
    "        print(\"-------------------------------------------------------------------------------------------------\") \n",
    "        icount +=1\n",
    "    \n",
    "    if icount > 3:\n",
    "        break \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5ff385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (922 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (863 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23,24,25,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (793 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27,28,Dataframe len= 10\n",
      "e_summ len= 29\n",
      "a_summ len= 29\n",
      "t5_summ len= 29\n",
      "ss_summ len= 29\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "e_list = []\n",
    "a_list = []\n",
    "t5_list = []\n",
    "sum_sum_list = []\n",
    "\n",
    "iCount = 0\n",
    "\n",
    "for article in merged_df['article_pp1']:\n",
    "    #print(article)\n",
    "    print(iCount, end=\",\")\n",
    "    iCount =  iCount + 1\n",
    "    \n",
    "    e_summary = generate_extractive_summary(article, min_summary_length=100)\n",
    "    a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "    t5_summary = generate_abstractive_summary_T5(article)\n",
    "    all_summary = e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "    s_s_summary = generate_abstractive_summary(all_summary, model = abstractive_summarizer_model)\n",
    "    \n",
    "    e_list.append(e_summary)\n",
    "    a_list.append(a_summary)\n",
    "    t5_list.append(t5_summary)\n",
    "    sum_sum_list.append(s_s_summary)\n",
    "\n",
    "\n",
    "print(\"Dataframe len=\", len(how2_df))\n",
    "print(\"e_summ len=\", len(e_list))\n",
    "print(\"a_summ len=\", len(a_list))\n",
    "print(\"t5_summ len=\", len(t5_list))\n",
    "print(\"ss_summ len=\", len(sum_sum_list))\n",
    "\n",
    "merged_df['e_summarization'] = e_list\n",
    "merged_df['a_summarization'] = a_list\n",
    "merged_df['t5_summarization'] = t5_list\n",
    "merged_df['ss_summarization'] = sum_sum_list\n",
    "\n",
    "merged_df.to_csv(os.getcwd() + \"/data/merged_df_with_Summarization.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ba03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a155e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue for Extractive Summarization\n",
      "\t\trouge-1:\tP:  3.80\tR:  2.15\tF1:  2.60\n",
      "\t\trouge-2:\tP:  0.34\tR:  0.31\tF1:  0.33\n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP:  3.73\tR:  2.23\tF1:  2.70\n",
      "\t\trouge-w:\tP:  2.20\tR:  0.52\tF1:  0.81\n",
      "Rogue for Abstractive Summarization\n",
      "\t\trouge-1:\tP: 25.31\tR: 20.91\tF1: 21.75\n",
      "\t\trouge-2:\tP:  3.61\tR:  3.15\tF1:  3.21\n",
      "\t\trouge-3:\tP:  0.07\tR:  0.07\tF1:  0.07\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP: 23.93\tR: 19.60\tF1: 20.76\n",
      "\t\trouge-w:\tP: 14.07\tR:  4.95\tF1:  7.02\n",
      "Rogue for T5 Summarization\n",
      "\t\trouge-1:\tP: 21.95\tR: 12.69\tF1: 15.13\n",
      "\t\trouge-2:\tP:  3.14\tR:  1.83\tF1:  2.19\n",
      "\t\trouge-3:\tP:  0.60\tR:  0.29\tF1:  0.37\n",
      "\t\trouge-4:\tP:  0.08\tR:  0.05\tF1:  0.06\n",
      "\t\trouge-l:\tP: 21.99\tR: 13.38\tF1: 15.91\n",
      "\t\trouge-w:\tP: 12.91\tR:  2.97\tF1:  4.62\n",
      "Rogue for SS Summarization\n",
      "\t\trouge-1:\tP: 24.44\tR: 21.25\tF1: 21.63\n",
      "\t\trouge-2:\tP:  2.83\tR:  2.77\tF1:  2.65\n",
      "\t\trouge-3:\tP:  0.03\tR:  0.06\tF1:  0.04\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP: 23.37\tR: 19.83\tF1: 20.69\n",
      "\t\trouge-w:\tP: 13.52\tR:  5.01\tF1:  7.01\n"
     ]
    }
   ],
   "source": [
    "#how2_df['e_summarization'] = e_list\n",
    "#how2_df['a_summarization'] = a_list\n",
    "#how2_df['t5_summarization'] = t5_list\n",
    "\n",
    "hypo=merged_df['summary'].tolist()\n",
    "refe1=merged_df['e_summarization'].tolist() #[reference]\n",
    "refe2=merged_df['a_summarization'].tolist() #[reference]\n",
    "refe3=merged_df['t5_summarization'].tolist() #[reference]\n",
    "refe6=merged_df['ss_summarization'].tolist() #[reference]\n",
    "\n",
    "print(\"Rogue for Extractive Summarization\")\n",
    "print_rogue_scores(hypo,refe1)    \n",
    "print(\"Rogue for Abstractive Summarization\")\n",
    "print_rogue_scores(hypo,refe2)   \n",
    "print(\"Rogue for T5 Summarization\")\n",
    "print_rogue_scores(hypo,refe3)        \n",
    "print(\"Rogue for SS Summarization\")\n",
    "print_rogue_scores(hypo,refe6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf487c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d7cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef9fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d977b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
