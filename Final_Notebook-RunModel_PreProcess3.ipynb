{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa1eed7",
   "metadata": {},
   "source": [
    "#  Running our models on the How2/WikiHow/CNN data. \n",
    "\n",
    "Following are the high level steps we are following in this notebook:\n",
    "* **Load Test Data :** Summary provided with the article.  \n",
    "* **Use PreProcessed3 data  :**  Pre-Processed 3 data has following details:\n",
    " * Remove Special Characters from Text\n",
    " * Remove Stop Words from Text\n",
    " * Lemmatize Text\n",
    " * Remove invalid and non-english words. \n",
    "* **Execute following Models  :**  We are executing multiple models including:\n",
    " * Extractive Summary Model (BERT)\n",
    " * Abstractive Summary Model (BERT2BERT for CNN/Dailymail)\n",
    " * Abstractive T5 Model (pre-trained model that was trained on our data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260a934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "##############\n",
    "## INSTALLS ##\n",
    "##############\n",
    "\n",
    "#!pip install bert-extractive-summarizer\n",
    "#!pip install transformers\n",
    "#!pip install neuralcoref\n",
    "#!pip install datasets==1.0.2\n",
    "#!pip install git-python==1.0.3\n",
    "#!pip install sacrebleu==1.4.12\n",
    "#!pip install rouge_score\n",
    "#!pip install rouge-metric\n",
    "\n",
    "#!pip install rouge\n",
    "#!pip install py-rouge\n",
    "#!pip install pyrouge\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece\n",
    "#!pip install nlp\n",
    "\n",
    "#!python -m nltk.downloader all\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30a36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_md\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import csv\n",
    "import rouge\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "#from rouge_score import rouge_scorer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from tqdm import tqdm_pandas\n",
    "from tqdm import tqdm\n",
    "from summarizer import Summarizer\n",
    "from simplet5 import SimpleT5\n",
    "from datetime import datetime\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2e99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package stopwords to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "###############\n",
    "# GLOBAL VARS #\n",
    "###############\n",
    "start_time = datetime.now()\n",
    "\n",
    "aggregator='Avg'\n",
    "apply_avg = aggregator == 'Avg'\n",
    "apply_best = aggregator == 'Best'\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")  \n",
    "abstractive_summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
    "extractive_summarizer_model = Summarizer()\n",
    "modelt5 = SimpleT5()\n",
    "modelt5.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9325fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c08ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>summary</th>\n",
       "      <th>article</th>\n",
       "      <th>data_source</th>\n",
       "      <th>article_pp1</th>\n",
       "      <th>article_pp2</th>\n",
       "      <th>article_pp3</th>\n",
       "      <th>num_words_article</th>\n",
       "      <th>num_sentences_article</th>\n",
       "      <th>num_words_summary</th>\n",
       "      <th>num_sentences_summary</th>\n",
       "      <th>num_words_article_pp1</th>\n",
       "      <th>num_sentences_article_pp1</th>\n",
       "      <th>num_words_article_pp2</th>\n",
       "      <th>num_sentences_article_pp2</th>\n",
       "      <th>num_words_article_pp3</th>\n",
       "      <th>num_sentences_article_pp3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>learn about how hand washing can help prevent ...</td>\n",
       "      <td>hi ! this is david jackel on behalf of expert ...</td>\n",
       "      <td>How2</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>most colds come from direct conotact that you ...</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>359</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>284</td>\n",
       "      <td>11</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>how to julienne cucumbers to make kimchi for k...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>How2</td>\n",
       "      <td>way cucumber also nice pickling cucumber find ...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>way cucumber also nice cucumber find work best...</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            summary  \\\n",
       "2           2  learn about how hand washing can help prevent ...   \n",
       "3           3  how to julienne cucumbers to make kimchi for k...   \n",
       "\n",
       "                                             article data_source  \\\n",
       "2  hi ! this is david jackel on behalf of expert ...        How2   \n",
       "3  the other way we can do cucumbers which is als...        How2   \n",
       "\n",
       "                                         article_pp1  \\\n",
       "2  cold come direct contact somebody else virus o...   \n",
       "3  way cucumber also nice pickling cucumber find ...   \n",
       "\n",
       "                                         article_pp2  \\\n",
       "2  most colds come from direct conotact that you ...   \n",
       "3  the other way we can do cucumbers which is als...   \n",
       "\n",
       "                                         article_pp3  num_words_article  \\\n",
       "2  cold come direct contact somebody else virus o...                359   \n",
       "3  way cucumber also nice cucumber find work best...                171   \n",
       "\n",
       "   num_sentences_article  num_words_summary  num_sentences_summary  \\\n",
       "2                     14                 20                      2   \n",
       "3                      6                 26                      2   \n",
       "\n",
       "   num_words_article_pp1  num_sentences_article_pp1  num_words_article_pp2  \\\n",
       "2                    123                          1                    284   \n",
       "3                     62                          1                    169   \n",
       "\n",
       "   num_sentences_article_pp2  num_words_article_pp3  num_sentences_article_pp3  \n",
       "2                         11                    116                          1  \n",
       "3                          6                     56                          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# DATA #\n",
    "########\n",
    "\n",
    "\n",
    "# setting number of rows to low number so notebook runs in minutes and not hours. \n",
    "num_rows_each_df = 10\n",
    "\n",
    "cnn_dailymail_df = pd.read_csv(os.getcwd() + \"/data/cnn_dm_df.csv\",encoding = \"utf-8\")\n",
    "wikihow_df = pd.read_csv(os.getcwd() + \"/data/wikihow_df.csv\",encoding = \"utf-8\")\n",
    "how2_df = pd.read_csv(os.getcwd() + \"/data/how2_df.csv\",encoding = \"utf-8\")\n",
    "\n",
    "wikihow_df = wikihow_df[(wikihow_df.article_pp1.str.len() < 3700) & (wikihow_df.summary.str.len() > 100)]\n",
    "how2_df = how2_df[(how2_df.article_pp1.str.len() < 3700) & (how2_df.summary.str.len() > 100)]\n",
    "cnn_dailymail_df = cnn_dailymail_df[(cnn_dailymail_df.article_pp1.str.len() > 250) & (cnn_dailymail_df.summary.str.len() > 100)]\n",
    "\n",
    "if len(wikihow_df) > num_rows_each_df:\n",
    "    wikihow_df = wikihow_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(how2_df) > num_rows_each_df:\n",
    "    how2_df = how2_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(cnn_dailymail_df) > num_rows_each_df:\n",
    "    cnn_dailymail_df = cnn_dailymail_df.head(num_rows_each_df)\n",
    "    \n",
    "merged_df = pd.concat([how2_df, wikihow_df,cnn_dailymail_df], axis=0)\n",
    "merged_df = merged_df[merged_df.article_pp1.str.len() > 250]\n",
    "\n",
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f0b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51419e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "def RemoveIntroFromText(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"the sentence is likely an introduction\")\n",
    "            new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def RemoveIntroFromTextMiddle(text):\n",
    "    script = nlp(text)\n",
    "    sentences = [x for x in script.sents]\n",
    "    #print(\"sentences.....\")\n",
    "    #print(sentences)\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"skipping the sentence as it is likely an introduction\")\n",
    "            #new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def RemoveIntroFromTextNonVerbose(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\" \n",
    "    #displacy.render(script, jupyter=True, style='ent') \n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        if len(d)>0:\n",
    "            for key in d:\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "             new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                 new_text+=\". \"\n",
    "        i+=1\n",
    "    return new_text\n",
    "\n",
    "\n",
    "#Raw Text Summarization\n",
    "def generate_abstractive_summary(raw_string, model = abstractive_summarizer_model, max_length=512):\n",
    "    \"\"\"This function produces an abstractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An abstractive summarizer model\"\"\"\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(raw_string, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_str[0]\n",
    "\n",
    "\n",
    "def generate_extractive_summary(raw_string, model = extractive_summarizer_model, min_summary_length = 50):\n",
    "    \"\"\"This function produces an extractive summary for a given article\"\n",
    "    Params:\n",
    "    raw_string: an article string.\n",
    "    model: An extractive summarizer model\"\"\"\n",
    "    output_str = model(raw_string, min_length = min_summary_length)\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def process_article(text):\n",
    "    #print(\"proces article\")\n",
    "    article = text.split(\".\")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        #print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(in_text, top_n=5):\n",
    "    summarize_text = []\n",
    "    try:\n",
    "        # Step 1 - Read text anc split it\n",
    "        sentences =  process_article(in_text)\n",
    "        # Step 2 - Generate Similary Martix across sentences\n",
    "        sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "        # Step 3 - Rank sentences in similarity martix\n",
    "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        # Step 4 - Sort the rank and pick top sentences\n",
    "        ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "        #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "        for i in range(top_n):\n",
    "            summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "        # Step 5 - Offcourse, output the summarize text\n",
    "        #print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "    except:\n",
    "        return \"\"\n",
    "    finally:\n",
    "        return \". \".join(summarize_text)\n",
    "\n",
    "def generate_abstractive_summary_T5(raw_string):\n",
    "    # using epoch 5\n",
    "    modelt5.load_model(\"t5\",\"outputs/simplet5-epoch-6-train-loss-1.5226\", use_gpu=False)\n",
    "    return modelt5.predict(raw_string)[0]\n",
    "\n",
    "#def prepare_results(p, r, f):\n",
    "#    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "def print_rogue_scores(hypo, refe):\n",
    "    scores = evaluator.get_scores(hypo, refe)\n",
    "    #scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "            for hypothesis_id, results_per_ref in enumerate(results):\n",
    "                nb_references = len(results_per_ref['p'])\n",
    "                for reference_id in range(nb_references):\n",
    "                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "                    print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results_per_ref['p'][reference_id], 'R', 100.0 * results_per_ref['r'][reference_id], 'F1', 100.0 * results_per_ref['f'][reference_id]))\n",
    "                    #print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "            print()\n",
    "        else:\n",
    "            print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))\n",
    "            #print(\"x\") #prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32a47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Rouge Evaluator  #\n",
    "####################\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b9373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Len= 758\n",
      "cold come direct contact somebody else virus often time like shaking hand somebody close quarter hugging touching anything close somebody else thing touching glass touching silverware touching food stuff like constantly washing hand especially traveling close quarter people exposed someone might sick always washing hand warm water soap wash vigorously least second make sure loosen germ wo always access warm water soap carry hand always keep hand car bag traveling something take whenever need put little bit hand shaking hand people public event know sick shaking hand someone else sick really worth getting sick use hand important thing remember hand vessel germ reach body always keep hand frequently possible also always wash hand eating touching face\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "e2-summary= \n",
      "----------------\n",
      "a-e-summary= hand vessel germs are common in the united states and the u. s. people are exposed to the cold come direct contact with a stranger. handilage a person's hand often and always wash hands with hand hygiene products. hand caricatures and hand washing are all common.\n",
      "----------------\n",
      "a-summary= hand vessel germs are common in the united states and the u. s. people are exposed to the cold come direct contact with a stranger. handilage a person's hand often and always wash hands with hand hygiene products. hand caricatures and hand washing are all common.\n",
      "----------------\n",
      "t5-summary= you might be exposed to the virus touching glass or silverware touching anything close to somebody else thing touching glass or silverware touching anything close to somebody else thing touching glass or silverware touching anything close to somebody else thing touching glass or silverware touching food stuff like constantly washing your hands always keep your hands always carry a warm water soap bottle and always wash your hands vigorously. Keep your hands always in your car bag when traveling and always wash your hands frequently especially when traveling close to people exposed to the virus Always shake your hands with your hands whenever possible\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 326\n",
      "way cucumber also nice cucumber find work best firmer slice diagonally thing cut little bit going use style well see going turn side going cut strip one going make nice soft flexible still good crunch careful cut finger ca enjoy meal finger missing thing end going bring bowl going mix together see process matter cut cucumber\n",
      "----------------\n",
      "e-summary= way cucumber also nice cucumber find work best firmer slice diagonally thing cut little bit going use style well see going turn side going cut strip one going make nice soft flexible still good crunch careful cut finger ca enjoy meal finger missing thing end going bring bowl going mix together see process matter cut cucumber\n",
      "----------------\n",
      "e2-summary= \n",
      "----------------\n",
      "a-e-summary= cucumber is good for a quick bite on the right side of the head. get a good look at the way cucumder is cut - edge. take a look at what cucumer is like for the first time... get ready to use style well.\n",
      "----------------\n",
      "a-summary= cucumber is good for a quick bite on the right side of the head. get a good look at the way cucumder is cut - edge. take a look at what cucumer is like for the first time... get ready to use style well.\n",
      "----------------\n",
      "t5-summary= see how to cut cucumber in the right way. Cut cucumber in the same way as the cucumber you want to cut it. Cut cucumber in half so that they are firmer and firmer. Cut cucumber in half so that they are firmer and firmer. Cut cucumber in half so that they are firmer and firmer. Cut cucumber in half so that they are firmer and firmer. Cut cucumber in half so that they are firmer and firmer. Cut cucumber in half so that they are firmer and firmer. Cut cucumber in half so\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 346\n",
      "photograph emulsion heat emulsion light tight space red photographic safe light paint photographic emulsion onto paper print traditional photographic technique dark room like especially make photograph necessarily flat plastic put paper make card make even really beautiful image one way kind create style photographic emulsion painted onto paper\n",
      "----------------\n",
      "e-summary= photograph emulsion heat emulsion light tight space red photographic safe light paint photographic emulsion onto paper print traditional photographic technique dark room like especially make photograph necessarily flat plastic put paper make card make even really beautiful image one way kind create style photographic emulsion painted onto paper\n",
      "----------------\n",
      "e2-summary= \n",
      "----------------\n",
      "a-e-summary= emulsion painted onto paper onto paper. traditional photographic technique is dark room like make - or - make - card paint painted on paper. use the emulsion to create a style photographic emulsion on paper and print. use this technique to make a perfect picture of your image or image.\n",
      "----------------\n",
      "a-summary= emulsion painted onto paper onto paper. traditional photographic technique is dark room like make - or - make - card paint painted on paper. use the emulsion to create a style photographic emulsion on paper and print. use this technique to make a perfect picture of your image or image.\n",
      "----------------\n",
      "t5-summary= paint on photographic emulsion. Paint on photographic emulsion Light tight space red photograph emulsion heat emulsion light tight space red photograph emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on photographic emulsion paint on paper make card\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 770\n",
      "episode actually going use interesting technique combine bleeding acupuncture along cupping commonly used heat syndrome well pain great choice back pain going swab area make sure sterile case use lancet similar diabetic use blood sugar utilize cup fashion place location use bleeding needle maybe able see camera starting get drop blood starting come forward leave minute amount blood start increase give moment work go actually starting get little bit drip good actually order therapeutic oftentimes leave five minute get teaspoon blood case magic go ahead leave quite long see release pressure cup blood actually spray cup cup might want go ahead watch video right much see little bit blood go ahead clean little bit bruising left bleeding cupping quite effective pain\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "e2-summary= \n",
      "----------------\n",
      "a-e-summary= use interesting technique combine bleeding acupuncture along with cup fashion placework. use blood sugar, caffeine and other medications to make sure sterile case works. take a look at some of the most common treatment options available in the united states and the u. s. to get your eye out of bleeding.\n",
      "----------------\n",
      "a-summary= use interesting technique combine bleeding acupuncture along with cup fashion placework. use blood sugar, caffeine and other medications to make sure sterile case works. take a look at some of the most common treatment options available in the united states and the u. s. to get your eye out of bleeding.\n",
      "----------------\n",
      "t5-summary= swab the back pain use a lancet or lancet for bleeding and cupping see how it works start with small drops of blood actually going to be sprayed on the affected area. Watch this video right now to see how you can do this in action and what happens next. Learn more about using a bleeding needle in this free video clip. Learn how to use a bleeding needle in this free video clip.\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TEST Loop for Abstractive and Extractive summarization\n",
    "\n",
    "icount = 0\n",
    "\n",
    "for article in merged_df['article_pp3']:\n",
    "    if len(article) > 200:\n",
    "        print(\"Article Len=\", len(article))\n",
    "        print(article)\n",
    "        e_summary = generate_extractive_summary(article, min_summary_length=50)\n",
    "        a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "        t5_summary = generate_abstractive_summary_T5(article)\n",
    "        all_summary = e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "        s_s_summary = generate_abstractive_summary(all_summary, model = abstractive_summarizer_model)\n",
    "        \n",
    "        print(\"----------------\")    \n",
    "        print(\"e-summary=\",e_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"a-summary=\",a_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"t5-summary=\",t5_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"ss-summary=\",s_s_summary)\n",
    "        print(\"-------------------------------------------------------------------------------------------------\") \n",
    "        icount +=1\n",
    "    \n",
    "    if icount > 3:\n",
    "        break \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5ff385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (968 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20,21,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23,24,25,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (900 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (595 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27,28,e_summ len= 29\n",
      "e2_summ len= 29\n",
      "a_e_summ len= 29\n",
      "a_summ len= 29\n",
      "t5_summ len= 29\n",
      "ss_summ len= 29\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "e_list = []\n",
    "a_list = []\n",
    "t5_list = []\n",
    "sum_sum_list = []\n",
    "\n",
    "iCount = 0\n",
    "\n",
    "for article in merged_df['article_pp3']:\n",
    "    #print(article)\n",
    "    print(iCount, end=\",\")\n",
    "    iCount =  iCount + 1\n",
    "    \n",
    "    e_summary = generate_extractive_summary(article, min_summary_length=100)\n",
    "    a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "    t5_summary = generate_abstractive_summary_T5(article)\n",
    "    all_summary = e_summary + \".\" + e2_summary + \".\" + a_e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "    s_s_summary = generate_abstractive_summary(all_summary, model = abstractive_summarizer_model)\n",
    "    \n",
    "    e_list.append(e_summary)\n",
    "    a_list.append(a_summary)\n",
    "    t5_list.append(t5_summary)\n",
    "    sum_sum_list.append(s_s_summary)\n",
    "    \n",
    "    #break \n",
    "\n",
    "\n",
    "print(\"e_summ len=\", len(e_list))\n",
    "print(\"a_summ len=\", len(a_list))\n",
    "print(\"t5_summ len=\", len(t5_list))\n",
    "print(\"ss_summ len=\", len(sum_sum_list))\n",
    "\n",
    "merged_df['e_summarization'] = e_list\n",
    "merged_df['a_summarization'] = a_list\n",
    "merged_df['t5_summarization'] = t5_list\n",
    "merged_df['ss_summarization'] = sum_sum_list\n",
    "\n",
    "merged_df.to_csv(os.getcwd() + \"/data/merged_df_with_Summarization.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ba03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a155e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue for Extractive Summarization\n",
      "\t\trouge-1:\tP:  3.53\tR:  2.14\tF1:  2.51\n",
      "\t\trouge-2:\tP:  0.14\tR:  0.15\tF1:  0.14\n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP:  3.52\tR:  2.24\tF1:  2.63\n",
      "\t\trouge-w:\tP:  2.06\tR:  0.53\tF1:  0.81\n",
      "Rogue for Extractive_2 Summarization\n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "Rogue for Abstractive Summarization\n",
      "\t\trouge-1:\tP: 24.15\tR: 19.61\tF1: 20.55\n",
      "\t\trouge-2:\tP:  2.87\tR:  2.34\tF1:  2.45\n",
      "\t\trouge-3:\tP:  0.10\tR:  0.14\tF1:  0.12\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP: 23.24\tR: 18.93\tF1: 20.15\n",
      "\t\trouge-w:\tP: 13.73\tR:  4.76\tF1:  6.80\n",
      "Rogue for Abstractive of Extractive Summarization\n",
      "\t\trouge-1:\tP: 24.15\tR: 19.61\tF1: 20.55\n",
      "\t\trouge-2:\tP:  2.87\tR:  2.34\tF1:  2.45\n",
      "\t\trouge-3:\tP:  0.10\tR:  0.14\tF1:  0.12\n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\t\trouge-l:\tP: 23.24\tR: 18.93\tF1: 20.15\n",
      "\t\trouge-w:\tP: 13.73\tR:  4.76\tF1:  6.80\n",
      "Rogue for T5 Summarization\n",
      "\t\trouge-1:\tP: 37.90\tR: 19.73\tF1: 24.67\n",
      "\t\trouge-2:\tP:  7.02\tR:  3.53\tF1:  4.47\n",
      "\t\trouge-3:\tP:  1.45\tR:  0.69\tF1:  0.91\n",
      "\t\trouge-4:\tP:  0.42\tR:  0.21\tF1:  0.28\n",
      "\t\trouge-l:\tP: 31.45\tR: 17.30\tF1: 21.55\n",
      "\t\trouge-w:\tP: 18.78\tR:  3.68\tF1:  5.96\n",
      "Rogue for SS Summarization\n",
      "\t\trouge-1:\tP: 27.45\tR: 21.96\tF1: 23.19\n",
      "\t\trouge-2:\tP:  3.59\tR:  2.67\tF1:  2.93\n",
      "\t\trouge-3:\tP:  0.56\tR:  0.37\tF1:  0.44\n",
      "\t\trouge-4:\tP:  0.15\tR:  0.12\tF1:  0.13\n",
      "\t\trouge-l:\tP: 25.09\tR: 20.25\tF1: 21.64\n",
      "\t\trouge-w:\tP: 15.05\tR:  5.14\tF1:  7.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hypo=merged_df['summary'].tolist()\n",
    "refe1=merged_df['e_summarization'].tolist() #[reference]\n",
    "refe2=merged_df['a_summarization'].tolist() #[reference]\n",
    "refe3=merged_df['t5_summarization'].tolist() #[reference]\n",
    "refe6=merged_df['ss_summarization'].tolist() #[reference]\n",
    "\n",
    "print(\"Rogue for Extractive Summarization\")\n",
    "print_rogue_scores(hypo,refe1)    \n",
    "print(\"Rogue for Abstractive Summarization\")\n",
    "print_rogue_scores(hypo,refe2)   \n",
    "print(\"Rogue for T5 Summarization\")\n",
    "print_rogue_scores(hypo,refe3)        \n",
    "print(\"Rogue for SS Summarization\")\n",
    "print_rogue_scores(hypo,refe6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75bf487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0d7cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:12:29.004730\n"
     ]
    }
   ],
   "source": [
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef9fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d977b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
