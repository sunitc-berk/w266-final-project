{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa1eed7",
   "metadata": {},
   "source": [
    "#  Running our models on the How2/WikiHow/CNN data. \n",
    "\n",
    "Following are the high level steps we are following in this notebook:\n",
    "* **Load Test Data :** Summary provided with the article.  \n",
    "* **Use PreProcessed2 data  :**  Pre-Processed 2 data has following details:\n",
    " * Remove Special Characters from Text\n",
    " * Remove double punctuations and cr-lf\n",
    " * Remove greeting words like 'hi', 'hello', ..\n",
    "* **Execute following Models  :**  We are executing multiple models including:\n",
    " * Extractive Summary Model (BERT)\n",
    " * Abstractive Summary Model (BERT2BERT for CNN/Dailymail)\n",
    " * Abstractive T5 Model (pre-trained model that was trained on our data). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260a934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "##############\n",
    "## INSTALLS ##\n",
    "##############\n",
    "\n",
    "#!pip install bert-extractive-summarizer\n",
    "#!pip install transformers\n",
    "#!pip install neuralcoref\n",
    "#!pip install datasets==1.0.2\n",
    "#!pip install git-python==1.0.3\n",
    "#!pip install sacrebleu==1.4.12\n",
    "#!pip install rouge_score\n",
    "#!pip install rouge-metric\n",
    "\n",
    "#!pip install rouge\n",
    "#!pip install py-rouge\n",
    "#!pip install pyrouge\n",
    "#!pip install torch\n",
    "#!pip install sentencepiece\n",
    "#!pip install nlp\n",
    "\n",
    "#!python -m nltk.downloader all\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30a36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "# IMPORTS #\n",
    "###########\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_md\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import csv\n",
    "import rouge\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "#from rouge_score import rouge_scorer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  \n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "from tqdm import tqdm_pandas\n",
    "from tqdm import tqdm\n",
    "from summarizer import Summarizer\n",
    "from simplet5 import SimpleT5\n",
    "from datetime import datetime\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2e99e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package stopwords to /home/sunitc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "###############\n",
    "# GLOBAL VARS #\n",
    "###############\n",
    "start_time = datetime.now()\n",
    "aggregator='Avg'\n",
    "apply_avg = aggregator == 'Avg'\n",
    "apply_best = aggregator == 'Best'\n",
    "vectorizer = TfidfVectorizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")  \n",
    "abstractive_summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
    "extractive_summarizer_model = Summarizer()\n",
    "modelt5 = SimpleT5()\n",
    "modelt5.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9325fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c08ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>summary</th>\n",
       "      <th>article</th>\n",
       "      <th>data_source</th>\n",
       "      <th>article_pp1</th>\n",
       "      <th>article_pp2</th>\n",
       "      <th>article_pp3</th>\n",
       "      <th>num_words_article</th>\n",
       "      <th>num_sentences_article</th>\n",
       "      <th>num_words_summary</th>\n",
       "      <th>num_sentences_summary</th>\n",
       "      <th>num_words_article_pp1</th>\n",
       "      <th>num_sentences_article_pp1</th>\n",
       "      <th>num_words_article_pp2</th>\n",
       "      <th>num_sentences_article_pp2</th>\n",
       "      <th>num_words_article_pp3</th>\n",
       "      <th>num_sentences_article_pp3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>learn about how hand washing can help prevent ...</td>\n",
       "      <td>hi ! this is david jackel on behalf of expert ...</td>\n",
       "      <td>How2</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>most colds come from direct conotact that you ...</td>\n",
       "      <td>cold come direct contact somebody else virus o...</td>\n",
       "      <td>359</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>284</td>\n",
       "      <td>11</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>how to julienne cucumbers to make kimchi for k...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>How2</td>\n",
       "      <td>way cucumber also nice pickling cucumber find ...</td>\n",
       "      <td>the other way we can do cucumbers which is als...</td>\n",
       "      <td>way cucumber also nice cucumber find work best...</td>\n",
       "      <td>171</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>in order to put photographic emulsion on water...</td>\n",
       "      <td>my name is anthony maddaloni and i 'm going to...</td>\n",
       "      <td>How2</td>\n",
       "      <td>photograph emulsion heat emulsion light tight ...</td>\n",
       "      <td>now photographs have an emulsion on them .and ...</td>\n",
       "      <td>photograph emulsion heat emulsion light tight ...</td>\n",
       "      <td>149</td>\n",
       "      <td>9</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>combining bleeding and cupping methods in acup...</td>\n",
       "      <td>in this episode , we 're actually going to use...</td>\n",
       "      <td>How2</td>\n",
       "      <td>episode actually going use interesting techniq...</td>\n",
       "      <td>in this episode , we 're actually going to use...</td>\n",
       "      <td>episode actually going use interesting techniq...</td>\n",
       "      <td>360</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>346</td>\n",
       "      <td>18</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>what terms are necessary for an umpire to know...</td>\n",
       "      <td>alright , some of the terminology is balls and...</td>\n",
       "      <td>How2</td>\n",
       "      <td>alright terminology ball strike call two ball ...</td>\n",
       "      <td>alright , some of the terminology is balls and...</td>\n",
       "      <td>alright terminology ball strike call two ball ...</td>\n",
       "      <td>177</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            summary  \\\n",
       "2           2  learn about how hand washing can help prevent ...   \n",
       "3           3  how to julienne cucumbers to make kimchi for k...   \n",
       "4           4  in order to put photographic emulsion on water...   \n",
       "5           5  combining bleeding and cupping methods in acup...   \n",
       "6           6  what terms are necessary for an umpire to know...   \n",
       "\n",
       "                                             article data_source  \\\n",
       "2  hi ! this is david jackel on behalf of expert ...        How2   \n",
       "3  the other way we can do cucumbers which is als...        How2   \n",
       "4  my name is anthony maddaloni and i 'm going to...        How2   \n",
       "5  in this episode , we 're actually going to use...        How2   \n",
       "6  alright , some of the terminology is balls and...        How2   \n",
       "\n",
       "                                         article_pp1  \\\n",
       "2  cold come direct contact somebody else virus o...   \n",
       "3  way cucumber also nice pickling cucumber find ...   \n",
       "4  photograph emulsion heat emulsion light tight ...   \n",
       "5  episode actually going use interesting techniq...   \n",
       "6  alright terminology ball strike call two ball ...   \n",
       "\n",
       "                                         article_pp2  \\\n",
       "2  most colds come from direct conotact that you ...   \n",
       "3  the other way we can do cucumbers which is als...   \n",
       "4  now photographs have an emulsion on them .and ...   \n",
       "5  in this episode , we 're actually going to use...   \n",
       "6  alright , some of the terminology is balls and...   \n",
       "\n",
       "                                         article_pp3  num_words_article  \\\n",
       "2  cold come direct contact somebody else virus o...                359   \n",
       "3  way cucumber also nice cucumber find work best...                171   \n",
       "4  photograph emulsion heat emulsion light tight ...                149   \n",
       "5  episode actually going use interesting techniq...                360   \n",
       "6  alright terminology ball strike call two ball ...                177   \n",
       "\n",
       "   num_sentences_article  num_words_summary  num_sentences_summary  \\\n",
       "2                     14                 20                      2   \n",
       "3                      6                 26                      2   \n",
       "4                      9                 54                      3   \n",
       "5                     16                 31                      3   \n",
       "6                     12                 27                      2   \n",
       "\n",
       "   num_words_article_pp1  num_sentences_article_pp1  num_words_article_pp2  \\\n",
       "2                    123                          1                    284   \n",
       "3                     62                          1                    169   \n",
       "4                     56                          1                    124   \n",
       "5                    127                          1                    346   \n",
       "6                     78                          1                    166   \n",
       "\n",
       "   num_sentences_article_pp2  num_words_article_pp3  num_sentences_article_pp3  \n",
       "2                         11                    116                          1  \n",
       "3                          6                     56                          1  \n",
       "4                          8                     48                          1  \n",
       "5                         18                    121                          1  \n",
       "6                         12                     68                          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########\n",
    "# DATA #\n",
    "########\n",
    "\n",
    "\n",
    "# setting number of rows to low number so notebook runs in minutes and not hours. \n",
    "num_rows_each_df = 10\n",
    "\n",
    "cnn_dailymail_df = pd.read_csv(os.getcwd() + \"/data/cnn_dm_df.csv\",encoding = \"utf-8\")\n",
    "wikihow_df = pd.read_csv(os.getcwd() + \"/data/wikihow_df.csv\",encoding = \"utf-8\")\n",
    "how2_df = pd.read_csv(os.getcwd() + \"/data/how2_df.csv\",encoding = \"utf-8\")\n",
    "\n",
    "wikihow_df = wikihow_df[(wikihow_df.article_pp1.str.len() < 3700) & (wikihow_df.summary.str.len() > 100)]\n",
    "how2_df = how2_df[(how2_df.article_pp1.str.len() < 3700) & (how2_df.summary.str.len() > 100)]\n",
    "cnn_dailymail_df = cnn_dailymail_df[(cnn_dailymail_df.article_pp1.str.len() > 250) & (cnn_dailymail_df.summary.str.len() > 100)]\n",
    "\n",
    "if len(wikihow_df) > num_rows_each_df:\n",
    "    wikihow_df = wikihow_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(how2_df) > num_rows_each_df:\n",
    "    how2_df = how2_df.head(num_rows_each_df)\n",
    "    \n",
    "if len(cnn_dailymail_df) > num_rows_each_df:\n",
    "    cnn_dailymail_df = cnn_dailymail_df.head(num_rows_each_df)\n",
    "    \n",
    "merged_df = pd.concat([how2_df, wikihow_df,cnn_dailymail_df], axis=0)\n",
    "#merged_df = pd.concat([how2_df, wikihow_df], axis=0)\n",
    "merged_df = merged_df[merged_df.article_pp1.str.len() > 250]\n",
    "#merged_df = merged_df.head(5000)\n",
    "#how2_df = how2_df.head(10)\n",
    "#how2_df = how2_df[(how2_df['num_words'] > 200)] # & (how2_df['num_words'] < 400 )]\n",
    "#wikihow_df = wikihow_df[(wikihow_df['num_words'] > 200)] # & (how2_df['num_words'] < 400 )]\n",
    "#cnn_dailymail_df = cnn_dailymail_df[(cnn_dailymail_df['num_words'] > 200)]# & (how2_df['num_words'] < 400 )]\n",
    "\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f0b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51419e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# HELPER FUNCTIONS #\n",
    "####################\n",
    "\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "def RemoveIntroFromText(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"the sentence is likely an introduction\")\n",
    "            new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def RemoveIntroFromTextMiddle(text):\n",
    "    script = nlp(text)\n",
    "    sentences = [x for x in script.sents]\n",
    "    #print(\"sentences.....\")\n",
    "    #print(sentences)\n",
    "    i=0\n",
    "    new_text=\"\"\n",
    "    print(\"Original text: \\n\")\n",
    "    displacy.render(script, jupyter=True, style='ent')\n",
    "    print(\"Some preprocessing details: \\n************\\n\")\n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        print(\"Sentence \", i, \": \", sentences[i])\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        print(d)\n",
    "        if len(d)>0:\n",
    "            print(d)\n",
    "            for key in d:\n",
    "                #print(\"key:\",key, \"; value=\", d[key])\n",
    "                #print(sent)\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (at_least_one_person>0):\n",
    "            print(\"the sentence has at least one person:\")\n",
    "            print(\"Sentence \", i, \": \", sentences[i])    \n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "            print(\"skipping the sentence as it is likely an introduction\")\n",
    "            #new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                print (\"Missing punctuation at the end\", sent, \"; last char is \", str(sent).strip()[-1])\n",
    "                new_text+=\". \"\n",
    "        i+=1\n",
    "    print(\"\\n*************\\nNew text, hopefully without person introduction:\\n**********\\n\", new_text)\n",
    "    return new_text\n",
    "\n",
    "def RemoveIntroFromTextNonVerbose(script):\n",
    "    sentences = [x for x in script.sents]\n",
    "    i=0\n",
    "    new_text=\"\" \n",
    "    #displacy.render(script, jupyter=True, style='ent') \n",
    "    is_intro=False\n",
    "    \n",
    "    for sent in sentences:\n",
    "        at_least_one_person=0\n",
    "        d= dict([(str(x), x.label_) for x in nlp(str(sent)).ents])\n",
    "        if len(d)>0:\n",
    "            for key in d:\n",
    "                if (d[key]==\"PERSON\"):\n",
    "                    at_least_one_person+=1\n",
    "        if \"expertvillage\" in str(sent).lower() or \"expert village\" in str(sent).lower():\n",
    "            is_intro=True\n",
    "        if (i<4 and (at_least_one_person>0  or is_intro)):\n",
    "             new_text=''\n",
    "        else:\n",
    "            new_text+=str(sent)\n",
    "            if not (str(sent).strip()[-1] in string.punctuation): \n",
    "                 new_text+=\". \"\n",
    "        i+=1\n",
    "    return new_text\n",
    "\n",
    "#Raw Text Summarization\n",
    "def generate_abstractive_summary(raw_string, model = abstractive_summarizer_model, max_length=512):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(raw_string, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return output_str[0]\n",
    "\n",
    "#This function produces an extractive summary for a given article\n",
    "def generate_extractive_summary(raw_string, model = extractive_summarizer_model, min_summary_length = 50):\n",
    "    output_str = model(raw_string, min_length = min_summary_length)\n",
    "    return output_str\n",
    "\n",
    "\n",
    "def process_article(text):\n",
    "    article = text.split(\".\")\n",
    "    sentences = []\n",
    "    for sentence in article:\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    return sentences\n",
    "\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    " \n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def generate_summary(in_text, top_n=5):\n",
    "    summarize_text = []\n",
    "    try:\n",
    "        # Step 1 - Read text anc split it\n",
    "        sentences =  process_article(in_text)\n",
    "        # Step 2 - Generate Similary Martix across sentences\n",
    "        sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "        # Step 3 - Rank sentences in similarity martix\n",
    "        sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "        scores = nx.pagerank(sentence_similarity_graph)\n",
    "        # Step 4 - Sort the rank and pick top sentences\n",
    "        ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "        #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "        for i in range(top_n):\n",
    "            summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "        # Step 5 - Offcourse, output the summarize text\n",
    "        #print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n",
    "    except:\n",
    "        return \"\"\n",
    "    finally:\n",
    "        return \". \".join(summarize_text)\n",
    "\n",
    "def generate_abstractive_summary_T5(raw_string):\n",
    "    # using epoch 6\n",
    "    modelt5.load_model(\"t5\",\"outputs/simplet5-epoch-6-train-loss-1.5226\", use_gpu=False)\n",
    "    return modelt5.predict(raw_string)[0]\n",
    "\n",
    "def print_rogue_scores(hypo, refe):\n",
    "    scores = evaluator.get_scores(hypo, refe)\n",
    "    #scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "            for hypothesis_id, results_per_ref in enumerate(results):\n",
    "                nb_references = len(results_per_ref['p'])\n",
    "                for reference_id in range(nb_references):\n",
    "                    print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "                    print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results_per_ref['p'][reference_id], 'R', 100.0 * results_per_ref['r'][reference_id], 'F1', 100.0 * results_per_ref['f'][reference_id]))\n",
    "                    #print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "            print()\n",
    "        else:\n",
    "            print('\\t' + '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * results['p'], 'R', 100.0 * results['r'], 'F1', 100.0 * results['f']))\n",
    "            #print(\"x\") #prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32a47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Rouge Evaluator  #\n",
    "####################\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b9373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Len= 1437\n",
      "most colds come from direct conotact that you 're having with somebody else who has the virus .often times that 's like shaking hands with somebody , being in close quarters , hugging , touching , anything when you 're close with somebody else , sharing things , touching the same glass , touching the same silverware , touching the same food , stuff like that .so what you should do is constanotly be washing your hands , especially if you 're traveling or if you 're in close quarters with people or you 're exposed to someone who might be sick .always be washing your hands and do it with warm water and soap , wash vigorously for at least 20 seconds to make sure you loosen up all the germs .now , you wo n't always have access to warm water and soap , so what you should do is carry hand sanitizer .i always keep some hand sanitizer with me , in my car , in a bag with me if i 'm traveling and this is something you can take out whenever you need to and just put a little bit of on your hands .if you 're shaking hands with people at a public evenot , you do n't know who is sick or who is shaking hands with someone else who is sick .it 's really not worth getting sick , so use some hand sanitizer .the importanot thing to remember is that hands are a vessel through which germs can reach your body , so always keep your hands as frequenotly as possible .also , always wash your hands before eating and before touching your face .\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "e2-summary= often times that 's like shaking hands with somebody , being in close quarters , hugging , touching , anything when you 're close with somebody else , sharing things , touching the same glass , touching the same silverware , touching the same food , stuff like that . now , you wo n't always have access to warm water and soap , so what you should do is carry hand sanitizer . also , always wash your hands before eating and before touching your face \n",
      "----------------\n",
      "a-e-summary= you should wash your hands before eating and before touching your face. don't always wash hands with warm water and soap, but you should do hand sanitizer. you should also wash the same glass of water and wash the other glass before eating. do you always wash your fingers before eating or before eating? share your tips on how to do it.\n",
      "----------------\n",
      "a-summary= most colds come from direct conotact that you're having with someone else. when shaking hands with somebody else, you do it with warm water and soap. use hand sanitizer to wash your hands and wash vigorously for at least 20 seconds to make sure you loosen up all the germs.\n",
      "----------------\n",
      "t5-summary= always be washing your hands, especially if you're traveling or in close quarters with people who might be sick. wash your hands frequenotly before eating and before touching your face.\n",
      "----------------\n",
      "ss-summary= wash your hands before eating and before touching your face. Wash your hands frequently before eating and before touching your face.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 807\n",
      "the other way we can do cucumbers which is also very nice is on these pickling cucumbers i find out that works out the best because t are firmer is to slice them diagonally .same thing about 1 8 '' to 1 4 '' you can cut them a little bit thicker if were going to use this style because well you 'll see why what were going to do once we 're doing this .turn them on the side we 're going to cut them inoto strips and that ones are pickled it 's going to make them nice soft and flexible but still have the good crunch .be careful not to cut your fingers because you ca n't enjoy your meal if your fingers are missing , same thing with the ends .we 're going to bring our bowl over and i 'm just going to mix them all together because as you see the process is the same no matter how you cut your cucumbers .\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "e2-summary= we 're going to bring our bowl over and i 'm just going to mix them all together because as you see the process is the same no matter how you cut your cucumbers . turn them on the side we 're going to cut them inoto strips and that ones are pickled it 's going to make them nice soft and flexible but still have the good crunch . same thing about 1 8 '' to 1 4 '' you can cut them a little bit thicker if were going to use this style because well you 'll see why what were going to do once we 're doing this \n",
      "----------------\n",
      "a-e-summary= the process is the same no matter how you cut your cucumbers. you can cut them a little thicker if you use this style. you'll see why what they were going to do once you're done. click here for all the latest from cnn. com / impact.\n",
      "----------------\n",
      "a-summary= pickling cucumbers inoto strips is the same thing as 1 8'to 1 4'' you can cut them a little bit thicker if your fingers are missing,'says t are firmer.'you'll see why what were going to do once we're done,'he says.\n",
      "----------------\n",
      "t5-summary= the other way you can do cucumbers is to slice them diagonally, but when cutting your cucumbers inoto strips, it's going to make them thicker and firmer.you can also use pickling cucumbers on these pickling cucumbers, just cut them diagonally and then chop them diagonally so that they are more firm and flexible.\n",
      "----------------\n",
      "ss-summary= the process for picking cucumbers is the same no matter how you cut them. pickling cucumbers inoto strips are 1 8' to 1 4'' thicker and firmer than pickling cucumbers inoto strips.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 675\n",
      "now photographs have an emulsion on them .and what i can do , is i can heat up this emulsion and light , tight space , using a red photographic safe light .and i can painot this photographic emulsion onoto watercolor paper .and then i can prinot using traditional photographic techniques in a dark room .and i like this especially because i can make photographs that are n't necessarily just flat on plastic .i can put them on watercolor paper , i can make cards out of them , i can make even really beautiful painoterly-esque images .and so that is one way that i can kind of create a style for myself using photographic emulsion that i 've painoted onoto watercolor paper .\n",
      "----------------\n",
      "e-summary= \n",
      "----------------\n",
      "e2-summary= and i can painot this photographic emulsion onoto watercolor paper . now photographs have an emulsion on them . and so that is one way that i can kind of create a style for myself using photographic emulsion that i 've painoted onoto watercolor paper \n",
      "----------------\n",
      "a-e-summary= the photographic emulsion onoto watercolor paper has an emulsion on them. onoto : \" i can painot this photographic paper. now i can kind of create a style for myself using emulsion that i've painoted onoto. \" he says he hopes to use the emulsion to create a personal style for himself.\n",
      "----------------\n",
      "a-summary= photographs have an emulsion on them. i can heat up this emulsion onoto watercolor paper. it's one way to create a style for myself using photographic techniques in a dark room. i like this especially because i can make images that aren't necessarily just flat on plastic.\n",
      "----------------\n",
      "t5-summary= i can heat up the photographic emulsion and light them in a tight space using a red photographic safe light.and then i can painot these photographs onoto watercolor paper with traditional photographic techniques in a dark room.\n",
      "----------------\n",
      "ss-summary= \"now i can kind of create a style for myself using the photographic emulsion that i've painoted onoto watercolor paper,\" he says.\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Article Len= 1627\n",
      "in this episode , we 're actually going to use an inoteresting technique that combines bleeding acupuncture along with cuppings , very commonly used for heat syndromes as well as for pain .so this is a great choice for back pain .once again , we 're going to swab the area to make sure that it 's sterile .in this case , we use a lancet which is very similar to a diabetic 's use if t 're checking their blood sugar .and now , we utilize the cup again in the same fashion and we 'll place that over the location we 're we just use the bleeding needle .and you maybe able to see from the camera we 're starting to get a drop of blood that 's starting to come forward , and if we leave on here for a few minutes , that amounot of blood will start to increase .we 'll give that a momenot to do some of its work .here we go .we 're actually starting to get a little bit of a drip from there which is good .and actually , in order to be therapeutic , we oftenotimes , we 'll leave this on for five minutes and get about a teaspoon or so of blood out of it .but in this case , for the magic of tv , we 'll go ahead and leave it there not quite so long , and what you 'll see is when i release the pressure from this cup , the blood actually kinda sprays up on the cup , cups .so if you 're screamish you might wanot to go ahead and not watch the video right now .ahh , we did n't have too much from that .so , you can see that we have a little bit of blood there. and we 'll go ahead and just clean that up and a little bit of bruising that 's left over from that .and that 's bleeding , cupping. and it 's quite effective for pain .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "e-summary= and we 'll go ahead and just clean that up and a little bit of bruising that 's left over from that .and that 's bleeding , cupping.\n",
      "----------------\n",
      "e2-summary= but in this case , for the magic of tv , we 'll go ahead and leave it there not quite so long , and what you 'll see is when i release the pressure from this cup , the blood actually kinda sprays up on the cup , cups . and actually , in order to be therapeutic , we oftenotimes , we 'll leave this on for five minutes and get about a teaspoon or so of blood out of it .  and we 'll go ahead and just clean that up and a little bit of bruising that 's left over from that \n",
      "----------------\n",
      "a-e-summary= we'll go ahead and clean that cup, cups, cups and soak up the blood. we can't wait for the magic of tv, but we can get about a teaspoon of blood out of it. we'd like to leave it on for five minutes and clean it up. we ’ ll get about five minutes of the tea spoon.\n",
      "----------------\n",
      "a-summary= in this episode we're going to use an inoteresting technique that combines bleeding acupuncture and cuppings. in this case, we use a lancet which is very similar to a diabetic's use if t's checking their blood sugar. we'll leave this on for five minutes and get about a teaspoon or so of that.\n",
      "----------------\n",
      "t5-summary= in this episode, we're going to use an inoteresting technique that combines bleeding acupuncture along with cuppings for pain.we're going to place a lancet over the area where we're using the cup and then we swab it with a lancet to get some blood out of it.\n",
      "----------------\n",
      "ss-summary= in this episode, we're going to use an inoteresting technique that combines bleeding acupuncture and cuppings for pain.\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TEST Loop for Abstractive and Extractive summarization\n",
    "\n",
    "icount = 0\n",
    "\n",
    "for article in merged_df['article_pp2']:\n",
    "    if len(article) > 200:\n",
    "        print(\"Article Len=\", len(article))\n",
    "        print(article)\n",
    "        e_summary = generate_extractive_summary(article, min_summary_length=50)\n",
    "        e2_summary = generate_summary(article, 3)\n",
    "        if len(e2_summary) > 5:\n",
    "            a_e_summary = generate_abstractive_summary(e2_summary, model = abstractive_summarizer_model) \n",
    "        elif len(e_summary) > 5:\n",
    "            a_e_summary = generate_abstractive_summary(e_summary, model = abstractive_summarizer_model) \n",
    "        else:\n",
    "            a_e_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model) \n",
    "\n",
    "        a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "        t5_summary = generate_abstractive_summary_T5(article)\n",
    "        all_summary = e_summary + \".\" + e2_summary + \".\" + a_e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "        s_s_summary = generate_abstractive_summary_T5(all_summary)\n",
    "        \n",
    "        print(\"----------------\")    \n",
    "        print(\"e-summary=\",e_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"e2-summary=\",e2_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"a-e-summary=\",a_e_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"a-summary=\",a_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"t5-summary=\",t5_summary)\n",
    "        print(\"----------------\")    \n",
    "        print(\"ss-summary=\",s_s_summary)\n",
    "        print(\"-------------------------------------------------------------------------------------------------\") \n",
    "        icount +=1\n",
    "    \n",
    "    if icount > 3:\n",
    "        break \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5ff385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (882 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (902 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (612 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (757 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (703 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2226 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (799 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1436 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1420 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (874 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1751 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1223 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (812 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe len= 10\n",
      "e_summ len= 29\n",
      "a_summ len= 29\n",
      "t5_summ len= 29\n",
      "ss_summ len= 29\n"
     ]
    }
   ],
   "source": [
    "# Generate summaries for the articles using the different approaches\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "e_list = []\n",
    "a_list = []\n",
    "t5_list = []\n",
    "sum_sum_list = []\n",
    "\n",
    "iCount = 0\n",
    "\n",
    "for article in merged_df['article_pp2']:\n",
    "    #print(article)\n",
    "    print(iCount, end=\",\")\n",
    "    iCount =  iCount + 1\n",
    "    \n",
    "    e_summary = generate_extractive_summary(article, min_summary_length=100)\n",
    "    a_summary = generate_abstractive_summary(article, model = abstractive_summarizer_model)\n",
    "    t5_summary = generate_abstractive_summary_T5(article)\n",
    "    all_summary = e_summary + \".\" + a_summary + \".\" + t5_summary + \".\"\n",
    "    s_s_summary = generate_abstractive_summary(all_summary, model = abstractive_summarizer_model)\n",
    "    \n",
    "    e_list.append(e_summary)\n",
    "    a_list.append(a_summary)\n",
    "    t5_list.append(t5_summary)\n",
    "    sum_sum_list.append(s_s_summary)\n",
    "\n",
    "\n",
    "print(\"Dataframe len=\", len(how2_df))\n",
    "print(\"e_summ len=\", len(e_list))\n",
    "print(\"a_summ len=\", len(a_list))\n",
    "print(\"t5_summ len=\", len(t5_list))\n",
    "print(\"ss_summ len=\", len(sum_sum_list))\n",
    "\n",
    "merged_df['e_summarization'] = e_list\n",
    "merged_df['a_summarization'] = a_list\n",
    "merged_df['t5_summarization'] = t5_list\n",
    "merged_df['ss_summarization'] = sum_sum_list\n",
    "\n",
    "merged_df.to_csv(os.getcwd() + \"/data/merged_df_with_Summarization.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ba03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a155e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue for Extractive Summarization\n",
      "\t\trouge-1:\tP: 30.99\tR: 19.04\tF1: 22.28\n",
      "\t\trouge-2:\tP:  6.76\tR:  3.53\tF1:  4.42\n",
      "\t\trouge-3:\tP:  2.55\tR:  1.19\tF1:  1.57\n",
      "\t\trouge-4:\tP:  1.34\tR:  0.58\tF1:  0.80\n",
      "\t\trouge-l:\tP: 25.10\tR: 15.85\tF1: 18.69\n",
      "\t\trouge-w:\tP: 14.69\tR:  3.52\tF1:  5.41\n",
      "Rogue for Abstractive Summarization\n",
      "\t\trouge-1:\tP: 28.89\tR: 23.81\tF1: 24.65\n",
      "\t\trouge-2:\tP:  5.76\tR:  4.66\tF1:  4.89\n",
      "\t\trouge-3:\tP:  1.85\tR:  1.37\tF1:  1.52\n",
      "\t\trouge-4:\tP:  0.73\tR:  0.47\tF1:  0.57\n",
      "\t\trouge-l:\tP: 24.68\tR: 20.42\tF1: 21.44\n",
      "\t\trouge-w:\tP: 14.74\tR:  5.23\tF1:  7.36\n",
      "Rogue for T5 Summarization\n",
      "\t\trouge-1:\tP: 30.20\tR: 31.76\tF1: 28.18\n",
      "\t\trouge-2:\tP:  5.99\tR:  6.78\tF1:  5.85\n",
      "\t\trouge-3:\tP:  1.87\tR:  2.01\tF1:  1.83\n",
      "\t\trouge-4:\tP:  0.80\tR:  0.79\tF1:  0.79\n",
      "\t\trouge-l:\tP: 27.06\tR: 27.53\tF1: 25.56\n",
      "\t\trouge-w:\tP: 16.43\tR:  8.23\tF1:  9.84\n",
      "Rogue for SS Summarization\n",
      "\t\trouge-1:\tP: 28.20\tR: 24.60\tF1: 24.93\n",
      "\t\trouge-2:\tP:  5.81\tR:  5.26\tF1:  5.35\n",
      "\t\trouge-3:\tP:  2.22\tR:  1.96\tF1:  2.05\n",
      "\t\trouge-4:\tP:  0.99\tR:  0.88\tF1:  0.93\n",
      "\t\trouge-l:\tP: 24.54\tR: 21.31\tF1: 21.97\n",
      "\t\trouge-w:\tP: 14.48\tR:  5.52\tF1:  7.64\n"
     ]
    }
   ],
   "source": [
    "# Calculate Rogue scores for the summarization that was just created. \n",
    "\n",
    "hypo=merged_df['summary'].tolist()\n",
    "refe1=merged_df['e_summarization'].tolist() #[reference]\n",
    "refe2=merged_df['a_summarization'].tolist() #[reference]\n",
    "refe3=merged_df['t5_summarization'].tolist() #[reference]\n",
    "refe4=merged_df['ss_summarization'].tolist() #[reference]\n",
    "\n",
    "print(\"Rogue for Extractive Summarization\")\n",
    "print_rogue_scores(hypo,refe1)    \n",
    "print(\"Rogue for Abstractive Summarization\")\n",
    "print_rogue_scores(hypo,refe2)  \n",
    "print(\"Rogue for T5 Summarization\")\n",
    "print_rogue_scores(hypo,refe3)        \n",
    "print(\"Rogue for SS Summarization\")\n",
    "print_rogue_scores(hypo,refe4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75bf487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0d7cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 0:11:04.749577\n"
     ]
    }
   ],
   "source": [
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef9fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d977b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
